 In lesson four, we will go deeper into architectures for big data, and we will take a closer look at some of the most popular big data management systems. First, we're going to look at how the big data management system framework looks, and explore the commonalities that pretty much all the big data systems have, as well as some of the key differences between no SQL, MPP, and Hadoop. Next, we're going to take a deep dive into the Hadoop data management system. You will see how we both store data in HDFS, the database, as well as how data analytics processing works with MapReduce. We will also give some examples, and you will see how the Apache ecosystem of projects helps to expand and connect Hadoop with other tools, including machine learning libraries and other data management systems. Finally, we will look at the evolution of Hadoop based on Yarn. Yarn is a quantum leap forward for Hadoop that allows it to support almost any type of deep analytics and machine learning framework. In this section, you will see how Yarn works, and how it supports external analytical applications. Okay, let's get started. When we look at the three V's of big data, it implies that there's going to be a big data management system of some kind. Now, for many people, when they think big data management system, the first word that pops to their mind is Hadoop. Now, Hadoop is, of course, a very, very popular system. It's very specialized in unstructured data and batch analytics. And you can buy distributions from companies like Cloudera, MapR, Hortonworks, and IBM. Now there are other systems, though, that solve different types of problems that is important to be aware of. For example, there's a type of big data management system called NoSQL. Now, that doesn't mean there's no SQL, but it means not only SQL. It's more of a flexible system that allows you to support both structured and unstructured data in the same system. An example of distributions would be Cassandra and Mongo. Another example is purely structured data on massive parallel processing systems or MPP. This would be examples of SAS and SAP HANA. SAP HANA, for example, uses MPP and an in-memory system, which makes it very, very fast because it doesn't have to go back to disk. So as you can see, there's a lot of variety and different types of big data management system. But in this lesson, we're going to take a little bit of a closer look at Hadoop. So let's take a look at some of the common characteristics of all the big data management systems. Well, typically they use local disk or local memory. Now, the reason they don't use a backend storage array or SAN is because of latency. Every time you have to read and write to a backend disk, it takes time. You're adding more and more latency. And if you have a really big analytics job you're trying to do, that time taken to read and write to bring data back and forth is going to consume too much time and just make the operation very, very slow. So we try to keep disk very, very local on those servers, which means the big data servers tend to have a lot of disk space. Now you can do also in memory systems, those tend to drive up a lot of the cost, but they are also local. Another commonality between no SQL, MPP, and Hadoop is that they all involve an element of parallel processing. These data sets are just too big to be managed by one computer. So parallel processing allows you to distribute the load and really create almost a supercomputer by networking all these computers together. And that network that's bringing them together is like the bus of that supercomputer. Another aspect of most of these big data management systems is that you're going to see a tremendous amount of East, West traffic, which means the nodes that hold all the data will be communicating with each other and passing a lot of information back and forth. This could be merging data sets, moving data around, replicating data, and different types of actions like this. Now that implies very, very high capacity network is going to be required as well. But altogether, these are some of the common attributes of all of the big data management systems. So now let's take a little bit of a closer look specifically at the Hadoop ecosystem. Now Hadoop is one of the best known and most deployed big data management systems out there. It's really the poster child of big data management. It's really built around distributed data systems of unstructured data or batch analytics. Now Hadoop was created by Doug Cutting and Michael Cafferalla while they were at Yahoo. And in 2006, it was donated to Apache. It is open source, but there's also a very, very large ecosystem and collection of different projects and plugins for Hadoop, which give it extra capabilities, which has made it very, very popular. Now it is traditionally focused on batch processing, which means it's not really well suited for real time processing, at least in its first version. But at least it is very, very scalable. In fact, you can put tens thousands of computers that all are part of the same processing job. There's really no fixed limitation in how much distributed and parallel processing that Hadoop is able to accomplish. It's also a very mature platform because it's been out there for many years and it's got a very big community of developers and it's very, very well documented. You might be wondering, where does this name Hadoop come from? Now, when you name a big software application that's going to be famous, the name is very, very important. Now, when Doug Cutting was working on this project and trying to figure out what name he should give this data analytics package, he was looking down at his two-year-old son that was playing with his little stuffed yellow elephant doll and he said, what is that called? And his son said, this is Hadoop Daddy. This is an actual picture of the real Hadoop doll. So that's where the name came from and actually the yellow elephant is the icon or the trademark logo of the Hadoop architecture. So Hadoop is popular for some very specific reasons. The first thing is that it is open source, which means that you don't have to necessarily pay for it. Now, most people don't just go and download Hadoop. They would buy distributions like Clodera or MapR or something else. But because it's open source, it allows developers to go and create software hooks and different applications that give extra capabilities to it. Secondly, it tends to be more affordable. Because you can use lower cost commodity servers to store the data and then perform the processing on that data, it allows you to put a tremendous number of them out there instead of having very, very high-powered and expensive servers with a ton of storage. Also, it is very, very scalable. You can add parallel processing almost in linear scale as much as you need. If you start to run out of data storage, all you have to do is just buy some more servers and you continually add them and you can continue to increase the performance and an overall storage of the Hadoop system. It is also a very fault tolerant system and this is really one of the things that has made it extremely powerful. It has an inbuilt capability to have redundant data sets. So if you lose an entire rack or a server, it's okay. There's actually redundancy built into the system, which means you're not going to lose your data. Hadoop is built on two foundational layers. One is the underlying database called the Hadoop Distributed File System or HDFS and then there's the processing layer. And commonly people think of MapReduce and then Yarn, which is also another abstraction layer that sits on top of that. But of course, Hadoop has more than that. There's this open-source projects that allow it to do extra things and they have kind of interesting names like Pig, Hive, Mahoot and many, many others. There's dozens of different open-source Hadoop projects and they're all built to do different things and to connect to different types of data sources. Now those three layers really encompass the Hadoop ecosystem, but there's also other external sources you might connect to and destinations. For example, what about data visualization or business intelligence? Those hooks into the Hadoop system that allow you to pull that data out and get the value from it. Those are not really part of the Hadoop open-source architecture, but they do work with the Hadoop system. Now we're going to focus specifically on those first two foundational layers, the database with HTFS and then the processing layer. First we're going to look at MapReduce and then we're going to look at Yarn. In the Hadoop architecture, there's two types of nodes. There's a master and a slave. Now on the master node, you have something called the name node. The name node is really the master where the data is orchestrated and it's sent down to the servers below. So name nodes are really responsible for coordinating where data is stored. The name node itself doesn't store the data. This is not part of the actual data accumulation and storage process, but it figures out where that data needs to go. Below it is a collection of data nodes and you're going to have many, many of these data nodes. This is where the data actually gets stored and the data blocks are written. Now this is under the direction of the name node. So he's going to talk to all the data nodes below, figure out where that data is going to get put and make sure the data is sent there and then replicate it in a very safe way and a redundant way down to those data nodes. The name nodes are typically stored in redundant fashion. So you'll have one or two of them or three of them for redundancy, whereas the data nodes you could have literally hundreds or even thousands of these. So let's look at the basics of how we would actually write a file block into the loop system. So let's say we have data that's coming in from its source and it's connecting to the master node. Now the name node function or the demon that sits on the master node has to select the data nodes below where the data is going to be written to. So we could have 64 or 128 meg blocks and the master node takes those and then figures out that it needs to write them down to the slaves. Now it does this by writing them down in redundant fashion to the slaves. So you have multiple copies of each data block. Now if we take an example of data that's coming in, we're going to see how replication actually works here. So we have the data coming in in different blocks. It hits the name node and he needs to keep a table of where that data lives. If you have thousands of servers sitting below and you're going to do a processing job on that data, you need to know where that data actually is so you can go and process it in an efficient way. So the name node keeps that data set on that table knowing exactly where the data is placed. So let's say we have one data block comes in and he's replicated in this case at four different locations. Now a minimum would be three. So we want to have very good triple redundancy, but in this case we're going to go even a step further and have four different data blocks stored on different servers. Another data block comes in, number two. Again, he's written two different servers. Again, a third data block comes in on the name node. We create a mapping and the name space. He's sent down to four different servers. Now as you can see, there is a very high level of redundancy built into the system. So do you think a Hadoop system would need a function like RAID or redundant array of independent disks, which is commonly used in server systems to protect the data? Well no, Hadoop does not use RAID. It doesn't need to. It has an inherently built data redundancy functionality where you've already got at least triple redundancy of every data block. So we would not use RAID in a Hadoop architecture. Now let's take a look at the processing function of how we actually execute analytics jobs on that data that's sitting in HDFS. So on the master node we have a new function, a new demon called the job tracker, and on the slave nodes we have a new one called the task tracker. Now let's say we have an application job that needs to communicate and analyze some data set that's sitting on the slave nodes down below. So the application job executes a Java command on the API, communicating with the name node, and then it tries to communicate down to the task trackers below. Now one of the big differences between big data architectures and traditional data processing is that we don't try to bring all the data to one place and analyze it. What we do is we send the processing job down to the data and distribute it. You can think of it like having a lot of minions doing the work for you. One analogy might be if you had a very, very big newspaper. Let's say you had a newspaper that was 10,000 pages long. And you wanted to find just one keyword in that newspaper. Well how would you do it? Well you could start on page one of that newspaper and read all the way through to page 10,000. And as you go you start to add up and count all the words that you're looking for. But that would take a very, very long time. But now what if you had a hundred friends and you give each of them one one hundredth of the newspaper? Now each friend is going to start at the beginning of his section of the newspaper and just count through. But all you have to do is basically collect and accumulate those results back together and just add them up. You've accomplished your job a hundred times faster by getting your hundred friends to help do the job for you. Well that's exactly what Hadoop is doing. With the job tap tracker and task tracker we're taking the complex analytics job and we're carving it up and splitting it up into small little sections and then we're sending it down to the minions to those task trackers to do the job for us. So really all we have to do then is collect the results back together, add them up and we will have the summary of our result and we can send it back to the application. So in this case we're going to communicate typically through Java to our master note. Now we could use something else like Hive or Python which will translate it to Java. But we're going to communicate that job. We're going to break it up by knowing where the data is and sending it down to the different slave nodes. Each task tracker takes his section of the job. He performs the analytics function on the data in his data node and then he returns the data back to the job tracker where it's accumulated, it's combined and finally the output is sent back to the application. And the more slave nodes we have the faster we can accomplish this. If we have a very small number of slave nodes well we're going to be in correlation to the processing on those slaves. So the more we have the faster we're going to be able to process our job and get the analytics result. So let's take a little bit of a closer look at what MapReduce is really doing here between the job tracker and the task tracker. Now on the far left here we're going to do a very similar job to that newspaper example that I just gave you. But we're going to look for a couple of keywords here. We're going to look for the words deer, bear, river and car. Now the first thing is we have an input. So that data is taken, it's split up and it's stored onto the HTFS layer down below. This is put into the slave nodes. So we have those words down there and we're going to execute an analytics job that looks for those words and tries to count them up. Now MapReduce functions here within this box. So the job tracker takes the analytics jobs and splits it up to the three different slave servers. And he executes each one accordingly. Now the first thing it does is create a key value pair. The key is the word and the value is the number of times that word appeared. For example on the first server we saw deer, one time, bear, one time and river one time. And we do a similar thing on the other servers. The next step is called shuffling. And this is where the task trackers communicate with each other and they start to see what each other has done to combine those results together. So this is where we start to see some east-west traffic communication between the different task trackers at the slave node level. Now once you have shuffled you started to combine and organize those results together. And finally you reduce those results by adding them and you combine them together where we have two bear words, three car words, two deer words and two river words. And finally the result is passed back to the application and we have the result we're looking for. So as you can see in this model we are actually doing a lot of east-west type of communication. So if we have a bunch of racks like this with data nodes that are communicating and they're comparing and shuffling results back and forth we're going to see this traffic going back and forth between the different nodes. As well even HTFS after the first write of that data block the first data node that takes it is going to replicate it to the other servers. So this east-west traffic is a very, very important thing and if you're working with the network engineer we have to make sure that the network itself is well architected where that communication doesn't become a bottleneck or we don't stop to drop packets and frames because of over subscription on that east-west communication. Let's just say we're doing a very, very complex map-produced job and this job is going to take us 8 to 10 hours to accomplish on our big data set. Well if we started to lose some functionality in the shuffling phase because of network congestion and some packet overflows and things like this the entire map-produced job could fail. Now we have data redundancy within the data blocks themselves but we do not have redundancy within the communication system we could have a cascading failure and the entire map-produced job would fail. This would mean that 8 to 10 hours of executing the map-produced job would be completely what waste of time. So it's very, very important to analyze how we've architected that network making sure that we have high availability, redundant paths and a lot of bandwidth between the different servers. HDFS and map-produced are two of the foundational elements of the Hadoop architecture but there's a lot of open source projects which add value to the whole system. For example one of them is called Uzi. This is a workflow coordination tool and lets you schedule map-produced jobs so you don't create a lot of bottlenecks. As I mentioned a batch job is like baking cookies. Once that batch of cookies goes into the oven you just have to wait for it to finish. Now if this is a data analytics job that's taking hours and hours and hours to do that becomes a bottleneck. Well how do we deal with bottlenecks like that? Well Uzi is an application that allows us to manage our map-produced jobs so that we don't create a lot of bottlenecks. There is another one called Mahoot. This is a machine learning library that lets you do a lot of the things we're going to learn about like regression and classification, deep neural networks on your data set. Now to be honest Mahoot is probably not one of the more popular machine learning frameworks but it is part of that open ecosystem of Hadoop projects so you can use it if you want but these days other projects and frameworks like PyTorch and TensorFlow have really become much more popular. There's one called Pig and as you can see all these different projects have very unusual and interesting names. It is a high level language that allows you to translate into map-produced speak. So map-produced can be a little bit confusing and building its analytic system and Pig allows you to really have a simple way to create those map-produced jobs and interestingly enough the language that Pig uses is called Pig Latin. Last one I'll talk about is Hive. Now as I mentioned Hadoop and HDFS is unstructured data which means there's no SQL down there. So what if you had to combine the data that's sitting at HDFS and you want to bring it together with a structured data system? It's not like no SQL that supports both structured and unstructured. Hadoop is strictly unstructured data. What Hive is is like a gateway or a bridge between the HDFS data architecture and another SQL-based system. Let's take an example here. Let's say I have SAP HANA which is an NPP-based SQL system and there's some data there that I want to combine with the data in HDFS and I'm going to bring these two data sets together to perform some big data analytics with machine learning function. Well these are typically isolated and orphan systems. How do I interconnect the two of them? Well this is where Hive comes in. Hive really becomes like the glue between two different systems. It allows me to combine the data in HDFS that's unstructured with a structured SQL-type relational data that might sit in Oracle or SQL Server or in SAP HANA. And by combining the two data sets I really enrich the overall value of the analytics process that I want to accomplish. What we've been discussing until now is really the first iteration of Hadoop, Hadoop 1.0, which comprises the HDFS database and MapReduce as the processing layer. Now the reality is there are some serious limitations with this architecture. It means that MapReduce and HDFS are very, very tightly tethered together. If you wanted to do any other type of application process on the data sitting in HDFS you couldn't do it. It's a way to do it through MapReduce to be able to get to that data. But MapReduce might not be able to do everything you want to do. There are some constraints around that that you just can't get around. The other thing is MapReduce is responsible for two key functions. It's the resource management and also the process management. Now the problem is these are so tightly coupled together that you had no ability to execute other types of applications sitting on that data. And over time people started to realize well this is actually a problem. What about streaming data or iterative type data? What about other applications like machine learning applications that are very, very specialized that I need to do on that data? I can't rely on what just MapReduce gives to me. Over time it was realized that Hadoop needed to mature and changes architecture to really open it up and decouple MapReduce from HDFS. And what we ended up with is the second generation of Hadoop. It's called yarn or yet another resource negotiator. What yarn is is really an abstraction layer that sits between the underlying database HDFS and the applications that sit above it. So MapReduce is one of those applications for batch analytics. But there also might be a lot of other applications. We could have real-time applications or anything else you can imagine. What yarn does, it separates and decouples MapReduce from the database and it manages the resources that sit below it and figures out where we need those applications to go. So yarn really started to change how Hadoop was being used. Yarn was first implemented by Yahoo back in 2013 and within a very short time they had tens of thousands of servers that were using yarn. It was using storm and spark and MapReduce for all different types of applications but on the same backend database. You see people really started to see the value of HDFS, the way it's implemented, it's fault tolerant capability. It's such a powerful database, it would be wonderful if you could execute other applications besides MapReduce on that data. And that's what yarn is all about. What yarn allows you to do is execute any type of application on that data and it can run in parallel, almost like virtualizing that data. So you can have an application like MapReduce or some other batch application that's executing on that data. You can have an interactive type of analytics job. You could use Spark, you could use Storm, HBase and any other type of application all together as different applications running as containers all on the same data that sits below. And there's obviously some economy of skill here. Because in this model, we're allowing ourselves to utilize the cluster in a much, much more efficient way. We're using the same server resources below but we're using multiple applications in that space. So there's going to be lower operational costs and you don't have to build an entirely separate data architecture for every application. It also reduces the amount of data that you need to move around because you're functioning on the same data set. You don't need to keep multiple copies of the data set and have them used for different applications. So how does yarn work? Well, we're going to take a little bit deeper look at this. There's four main elements in the yarn architecture. The first is the resource manager or RM. Now this manages the resources of the entire cluster. Now in the past, that was done by MapReduce. But here we separated out to a separate demon specifically looking at the resources in the cluster. Now resource manager has two subfunctions which is a scheduler and an app manager. We're going to take a closer look exactly what these do. But you can imagine a client coming in connecting to the client service on the resource manager and then he figures out how to handle that application and that client and then process that job. Now in this example, we see two different clients which means two different types of applications that are talking to the RM and they want to run on the data space below. The second component is called the app master and this lives down on the data nodes. Now there's one application master per application. So if I had 50 or 100 data nodes below, one of those nodes is going to be allocated as the app master for that application. Now we might be running it on all 50 or 100 data nodes but there's only one master that typically lives down on that data node below. We also have the node manager. The node manager is part of the data node. He manages the application space and the containers and looks at the resources that are functioning on that data node and he communicates it back to the RM to express what the CPU cycles are like, the data storage, clock cycles, security tokens and all this kind of information is passed between the node manager and the resource manager. Finally we have the application containers themselves. So this is where the application job actually runs and actually lives. Now the next thing about it, it's not constrained by a particular programming language or any other construct. It's a container. You can write it in whatever language you like and it can do whatever you want it to do. So you can use things like TensorFlow or PyTorch or other types of frameworks and create an application to write it on your HDFS data set sitting below. Let's take a little bit of a closer look at the resource manager. So the RM is really the orchestrator or the manager of the overall process. Typically you'll have one of these active at any given time but you would deploy the resource manager in redundant fashion. It's also possible to use the ZOOK Keeper Elector function if you wanted to have multiple cluster of the RM's but in most cases they're deployed in an active standby type of mode. Now the resource manager has two core components, the scheduler and the application manager. Notice the application manager. He keeps a list of all the applications that are running on that cluster. Now he doesn't manage those applications specifically but he keeps a list of them and he knows what's running in the network. He doesn't pull them to see their activity and try to figure out if it's on the best server. He just is aware of them so he knows where they are. So when a client comes in and says I need to execute an application on the data, that application is added to the app manager and he is then part of the overall system. The second component is the scheduler. This is where we look at the actual resources down on the data nodes below and we figure out where we're going to actually install that application container to let it run on those servers. So the scheduler actually has a very, very important job. He needs to figure out where is the data, what is the best server to actually run this on and how are we going to perform it and monitor it to make sure that it's actually running as it should. The second major component is the application master. Now again we're going to have one application master for every application. So if you have 10 different applications that are running on your data set, you will have 10 different application masters spread on your data nodes down below. The application master communicates both to the resource manager as well to the node manager. So let's say an application comes in and he wants to execute. Well the first thing that's created is the application master container. He starts to look at the requirements of that application and he tries to figure out how much CPU it needs, how much memory and how much disk space. It's security tokens. And he takes that information and he goes and he talks to the RM and he says, this is what I need for my application. The RM then talks to the node manager and looks at the different data nodes below to see where the best place to install that application container would be. Once he's done that, he allocates a lease for that container down to the application master who then communicates to the node manager who spins up that container and it starts to function. So the app master is really providing now a container launch context or a CLC that includes the environment variables, dependencies and all that other information that was communicated from the RM so he knows exactly how to set up that container. A third major component of the RM is the node manager. Now the node manager runs on every single node and he communicates with the app master. He monitors the overall health of the node, he sets up a container environment and he really looks at what needs to be done to maintain it. He also needs to communicate back to the RM to make sure that the node manager's health is reported so that the RM when he schedules new jobs, he's sending it to the right node. If a node manager starts to experience performance issues and the application is not running, well, that needs to be communicated back to the RM so that when he schedules future jobs that they don't go to that node because he might be suffering some performance issues. The node manager is the one that creates the application's container based on its lease and the container launch context or the CLC based on that information he gets from the app master. And once it's running, now we typically would have an application container running on every node manager where we've allocated it from the RM. Now, let's take a little bit of an example and step through exactly how this works step-by-step. Now here we see a Hadoop cluster. We have HDFS with data nodes and this data has been written to a whole bunch of different servers. Now on the left hand side we have our resource manager. We begin with a client who's executing a Hadoop job and he communicates into the RM. Now this is an application that's going to execute. The RM has to figure out where am I going to schedule these jobs and launch those application containers. So the first thing we need to do is use the app manager on the RM to create a container that we start this process. So he picks one of the data nodes and he launches this application master container. And again, we'll have one of these for every application. The app master now looks at the requirements of that application and he figures out exactly what he needs to run. Now the app master will go back to the RM and communicate the resource requirements and says exactly this is what I need. The RM in turn will go to the data nodes and look at them and figure out which one can meet the needs that's being asked for by the app master. Once he's done this he creates a lease which is sent back to the app master. The app master now has his CLC where he can talk to the node managers to spin up those application containers on every single node manager that's been allocated. He now talks to the node managers and tells them to spin up a new container with the applications installed and it begins to run. And in this way we continue to run different applications until finally it expires and is deleted and then we can remove it and then the application has finished its analytics job.