{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce03f286-3b5c-45a8-94d2-2d1f749f2ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "INFO:root:Processing document: docs/War-of-the-Worlds.pdf\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:root:Chroma vectorstore initialized in 0.26 seconds.\n",
      "INFO:root:Document loaded in 0.48 seconds.\n",
      "INFO:root:Document split into chunks in 0.00 seconds.\n",
      "INFO:root:Document embedded and added to vectorstore in 762.85 seconds.\n",
      "INFO:root:Chroma vectorstore persisted in 0.02 seconds.\n",
      "INFO:root:Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# You still need (if RAG) a vector database. Using this code as a reminder. Jump to block 4 below unless you need to reinstall the database.\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Optional: Disabling warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Configuration\n",
    "FILE_PATH = \"docs/War-of-the-Worlds.pdf\"\n",
    "CHROMA_PERSIST_DIR = \"chroma_store_chatbot\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# Function to load and split the PDF document\n",
    "def load_and_split_document(filepath):\n",
    "    \"\"\"Load the PDF file, split it into chunks, and return chunks with metadata.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    loader = PyPDFLoader(filepath)\n",
    "    documents = loader.load()  # This returns a list of Document objects\n",
    "    logging.info(f\"Document loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Split document into chunks of 1000 characters with a 100-character overlap\n",
    "    start_time = time.time()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    split_documents = text_splitter.split_documents(documents)  # Pass the list directly\n",
    "    logging.info(f\"Document split into chunks in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    # Add metadata to each chunk\n",
    "    for doc in split_documents:\n",
    "        doc.metadata[\"source\"] = filepath\n",
    "    \n",
    "    return split_documents\n",
    "\n",
    "# Embed and store the document if not already processed\n",
    "def embed_and_store_document():\n",
    "    \"\"\"Load, embed, and store the document in Chroma.\"\"\"\n",
    "    logging.info(f\"Processing document: {FILE_PATH}\")\n",
    "    \n",
    "    # Initialize the Ollama embedding function\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "    \n",
    "    # Initialize or load the existing Chroma vectorstore with the embedding function\n",
    "    start_time = time.time()\n",
    "    vectorstore = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embeddings)\n",
    "    logging.info(f\"Chroma vectorstore initialized in {time.time() - start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Check if the document has already been processed\n",
    "    existing_files = set(metadata.get(\"source\") for metadata in vectorstore.get()[\"metadatas\"])\n",
    "    \n",
    "    if FILE_PATH in existing_files:\n",
    "        logging.info(f\"Document already processed: {FILE_PATH}\")\n",
    "        return\n",
    "    \n",
    "    # Load and split the document into chunks\n",
    "    document_chunks = load_and_split_document(FILE_PATH)\n",
    "    \n",
    "    # Embed and add the new document's chunks to the vectorstore\n",
    "    start_time = time.time()\n",
    "    vectorstore.add_documents(document_chunks)\n",
    "    logging.info(f\"Document embedded and added to vectorstore in {time.time() - start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Persist the updated vector store\n",
    "    start_time = time.time()\n",
    "    vectorstore.persist()\n",
    "    logging.info(f\"Chroma vectorstore persisted in {time.time() - start_time:.2f} seconds.\")\n",
    "    \n",
    "    logging.info(\"Processing complete.\")\n",
    "\n",
    "# Run the embedding and storing process for a single document\n",
    "embed_and_store_document()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b552444-32e3-4a09-8eea-984628934793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma database at 'chroma_store_chatbot' has been cleared.\n"
     ]
    }
   ],
   "source": [
    "# In case of need, cleaning and resetting the chromadb so you can do this exercize multiple times\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "CHROMA_PERSIST_DIR = \"chroma_store_chatbot\"\n",
    "\n",
    "def clear_chroma_database():\n",
    "    \"\"\"Delete the Chroma database directory to start from scratch.\"\"\"\n",
    "    if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "        # Remove the entire directory and its contents\n",
    "        shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "        print(f\"Chroma database at '{CHROMA_PERSIST_DIR}' has been cleared.\")\n",
    "    else:\n",
    "        print(f\"No Chroma database found at '{CHROMA_PERSIST_DIR}' to clear.\")\n",
    "\n",
    "# Run the function to clear the Chroma database\n",
    "clear_chroma_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a2b2ae9-d1dd-499b-a988-c40d3ba15050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We still need these bricks, so do not run this part of the notebook in isolation\n",
    "persist_directory = \"chroma_store_chatbot\"\n",
    "embedding = OllamaEmbeddings(model=\"llama3\")\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6b3f55-a558-4082-8cbd-731dfab87e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n"
     ]
    }
   ],
   "source": [
    "# Let's start here, by checking that the vectordb still has the 443 chunks of the pdf document.\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87283787-82d9-4c20-a39b-b3e71a04443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And le'ts bring back the context from the previous notebook.\n",
    "question = \"Did the spaceship come from the planet Mars?\"\n",
    "docs = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)\n",
    "#Using Llama3 as the LLM, and Ollama as the wrapper to interact with Llama3\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model = \"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d60cc3b1-d035-4f4c-a5cb-bd875a0d48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama\n",
    "#!ollama serve & ollama pull llama3 & ollama pull nomic-embed-text\n",
    "#!pip install ollama langchain beautifulsoup4 chromadb gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c333776-43f2-4538-8214-6d15ce7a032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/gradio-messaging/en \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7867/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7867/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Let's wrap the RAG into a simple UI. \n",
    "import gradio as gr\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Create Ollama embeddings and vector store\n",
    "#embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "#vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Define the function to call the Ollama Llama3 model\n",
    "def ollama_llm(question, context):\n",
    "    # Explicitly create a new conversation with only the current prompt\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='llama3', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "# Define the RAG setup\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context)\n",
    "\n",
    "# Define the Gradio interface\n",
    "def get_important_facts(question):\n",
    "    return rag_chain(question)\n",
    "\n",
    "# Create a Gradio app interface\n",
    "iface = gr.Interface(\n",
    "  fn=get_important_facts,\n",
    "  inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "  outputs=\"text\",\n",
    "  title=\"RAG with Llama3\",\n",
    "  description=\"Ask questions\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n",
    "# example q: did the aliens eventually go on to land on Venus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54fea6a2-1f71-445d-8af1-6cd91a1661fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7868/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7868/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Now, Ollama re-injects the previous question and asnwer into the model with the next question. But other LLMs would forget the previous question (remember Dolly?) You can add memory with a memory module.\n",
    "# Let's also add a debug function to show what was passed to the LLM and the retriever.\n",
    "import gradio as gr\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define memory to store the previous exchanges\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Custom Runnable LLM class with debugging\n",
    "class OllamaLLM(Runnable):\n",
    "    def __init__(self, llm_fn):\n",
    "        self.llm_fn = llm_fn\n",
    "\n",
    "    def invoke(self, input, config=None, **kwargs):\n",
    "        # If the input is a StringPromptValue or similar object, treat it as a string\n",
    "        question = str(input)  # Convert the input to a string if necessary\n",
    "        context = kwargs.get(\"context\", \"\")  # Retrieve context from kwargs if available\n",
    "\n",
    "        # Print what was passed to the LLM\n",
    "        print(f\"Question passed to LLM: {question}\")\n",
    "        print(f\"Context passed to LLM: {context}\")\n",
    "\n",
    "        # Handle additional kwargs such as stop, if needed\n",
    "        stop = kwargs.get(\"stop\", None)\n",
    "\n",
    "        # If 'stop' or other arguments need to be passed to the LLM function, handle them here\n",
    "        response = self.llm_fn(question, context)\n",
    "        \n",
    "        # Print the response from the LLM\n",
    "        print(f\"Response from LLM: {response}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    def predict(self, input, **kwargs):\n",
    "        return self.invoke(input, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.invoke(*args, **kwargs)\n",
    "\n",
    "# Instantiate the custom LLM class\n",
    "ollama_llm_instance = OllamaLLM(ollama_llm)\n",
    "\n",
    "# Define the conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ollama_llm_instance,\n",
    "    retriever=retriever,\n",
    "    memory=memory # add the memory module, to pass the previous exchange to the LLM as well\n",
    ")\n",
    "\n",
    "# Define the function to get important facts with debugging\n",
    "def get_important_facts(question):\n",
    "    # Print what is passed to the retriever\n",
    "    print(f\"Question passed to retriever: {question}\")\n",
    "    \n",
    "    # Run the chain and capture the memory state\n",
    "    response = qa_chain.run({\"question\": question})\n",
    "    \n",
    "    # Print what is in memory after the retrieval\n",
    "    print(f\"Memory state: {memory.buffer}\")\n",
    "\n",
    "    return response\n",
    "\n",
    "# Create a Gradio app interface\n",
    "iface = gr.Interface(\n",
    "    fn=get_important_facts,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"RAG with Llama3\",\n",
    "    description=\"Ask questions\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453d737-2dee-4a44-bc19-ed1efca52857",
   "metadata": {},
   "source": [
    "## Chains and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32edc7b-16cd-4e4d-bffd-7169aacf2a8e",
   "metadata": {},
   "source": [
    "You may also need your LLM to retrieve information from elsewhere. This is where LangChain tools and chains become useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27d85d83-3f45-4473-b9fd-1db6b5028d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a simple function that returns the weather for a location\n",
    "import sys\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenWeatherMap API key from the environment variables\n",
    "OPENWEATHER_API_KEY = os.environ.get('OPENWEATHER_API_KEY') # set a keys.env file in the parent directory, and set there your openweather API key\n",
    "\n",
    "from langchain.agents import tool\n",
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the input schema\n",
    "class CityInput(BaseModel):\n",
    "    city: str = Field(..., description=\"City name to fetch weather data for\")\n",
    "\n",
    "# Tool to get the current weather\n",
    "@tool(args_schema=CityInput)\n",
    "def get_current_weather(city: str) -> dict:\n",
    "    \"\"\"Fetch current weather for a given city.\"\"\"\n",
    "\n",
    "    BASE_URL = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    \n",
    "    # Parameters for the weather request\n",
    "    params = {\n",
    "        'q': city,\n",
    "        'appid': OPENWEATHER_API_KEY,\n",
    "        'units': 'metric',  # To get the temperature in Celsius\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        weather_data = {\n",
    "            \"City\": city,\n",
    "            \"Temperature\": f\"{results['main']['temp']}°C\",\n",
    "            \"Weather Description\": results['weather'][0]['description'],\n",
    "            \"Humidity\": f\"{results['main']['humidity']}%\",\n",
    "            \"Wind Speed\": f\"{results['wind']['speed']} m/s\",\n",
    "            \"Pressure\": f\"{results['main']['pressure']} hPa\"\n",
    "        }\n",
    "    else:\n",
    "        raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "    return weather_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48513d89-2fca-48d8-b2c2-48f893fa8a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'City': 'Richmond',\n",
       " 'Temperature': '23.22°C',\n",
       " 'Weather Description': 'broken clouds',\n",
       " 'Humidity': '73%',\n",
       " 'Wind Speed': '4.12 m/s',\n",
       " 'Pressure': '1015 hPa'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_weather(\"Richmond\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c23f6f2-a2d3-4668-bfce-107694c10bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even without an LLM, we can of course build a nice sentence for the response\n",
    "from langchain.agents import tool\n",
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the input schema\n",
    "class CityInput(BaseModel):\n",
    "    city: str = Field(..., description=\"City name to fetch weather data for\")\n",
    "\n",
    "# Tool to get the current weather\n",
    "@tool(args_schema=CityInput)\n",
    "def get_current_weather(city: str) -> dict:\n",
    "    \"\"\"Fetch current weather for a given city.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    \n",
    "    # Parameters for the weather request\n",
    "    params = {\n",
    "        'q': city,\n",
    "        'appid': OPENWEATHER_API_KEY,\n",
    "        'units': 'metric',  # To get the temperature in Celsius\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        temperature = results['main']['temp']\n",
    "        weather_description = results['weather'][0]['description']\n",
    "        humidity = results['main']['humidity']\n",
    "        wind_speed = results['wind']['speed']\n",
    "        pressure = results['main']['pressure']\n",
    "    else:\n",
    "        raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "    return (\n",
    "        f\"The current temperature in {city} is {temperature}°C with {weather_description}.\\n\"\n",
    "        f\"Humidity: {humidity}%, Wind Speed: {wind_speed} m/s, Pressure: {pressure} hPa.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaa3d5ad-13cc-4c06-a374-5f8316fe34d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Richmond is 23.22°C with broken clouds.\\nHumidity: 73%, Wind Speed: 4.12 m/s, Pressure: 1015 hPa.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_weather(\"Richmond\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d58d3ca8-7f3c-4363-abb5-bbe888e71f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7869/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7869/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "#Integrating the weather in the full code\n",
    "\n",
    "import gradio as gr\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.runnables import Runnable\n",
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Weather tool function\n",
    "def get_current_weather(city: str) -> str:\n",
    "    \"\"\"Fetch current weather for a given city.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    \n",
    "    params = {\n",
    "        'q': city,\n",
    "        'appid': OPENWEATHER_API_KEY,\n",
    "        'units': 'metric',  # To get the temperature in Celsius\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        temperature = results['main']['temp']\n",
    "        weather_description = results['weather'][0]['description']\n",
    "        humidity = results['main']['humidity']\n",
    "        wind_speed = results['wind']['speed']\n",
    "        pressure = results['main']['pressure']\n",
    "    else:\n",
    "        raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "    return (\n",
    "        f\"The current temperature in {city} is {temperature}°C with {weather_description}.\\n\"\n",
    "        f\"Humidity: {humidity}%, Wind Speed: {wind_speed} m/s, Pressure: {pressure} hPa.\"\n",
    "    )\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define memory to store the previous exchanges\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Custom Runnable LLM class with debugging\n",
    "class OllamaLLM(Runnable):\n",
    "    def __init__(self, llm_fn):\n",
    "        self.llm_fn = llm_fn\n",
    "\n",
    "    def invoke(self, input, config=None, **kwargs):\n",
    "        question = str(input)\n",
    "        context = kwargs.get(\"context\", \"\")\n",
    "        print(f\"Question passed to LLM: {question}\")\n",
    "        print(f\"Context passed to LLM: {context}\")\n",
    "        stop = kwargs.get(\"stop\", None)\n",
    "        response = self.llm_fn(question, context)\n",
    "        print(f\"Response from LLM: {response}\")\n",
    "        return response\n",
    "\n",
    "    def predict(self, input, **kwargs):\n",
    "        return self.invoke(input, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.invoke(*args, **kwargs)\n",
    "\n",
    "# Instantiate the custom LLM class\n",
    "ollama_llm_instance = OllamaLLM(ollama_llm)\n",
    "\n",
    "# Define the conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ollama_llm_instance,\n",
    "    retriever=retriever,\n",
    "    memory=memory  # add the memory module to pass the previous exchange to the LLM as well\n",
    ")\n",
    "\n",
    "# Function to detect if the question is about the weather in a city\n",
    "def is_weather_question(question: str) -> bool:\n",
    "    return \"weather\" in question.lower() and \"in\" in question.lower()\n",
    "\n",
    "# Extract city name from the weather question\n",
    "def extract_city_from_question(question: str) -> str:\n",
    "    # Simple heuristic to extract city name\n",
    "    if \"weather in\" in question.lower():\n",
    "        return question.lower().split(\"weather in\")[1].strip().split()[0].capitalize()\n",
    "    return \"\"\n",
    "\n",
    "# Define the function to get important facts with debugging\n",
    "def get_important_facts(question):\n",
    "    # Check if the question is about the weather in a city\n",
    "    if is_weather_question(question):\n",
    "        city = extract_city_from_question(question)\n",
    "        if city:\n",
    "            return get_current_weather(city)\n",
    "        else:\n",
    "            return \"I couldn't determine the city you're asking about. Please specify the city.\"\n",
    "    \n",
    "    # Otherwise, use the LLM-based chain\n",
    "    print(f\"Question passed to retriever: {question}\")\n",
    "    response = qa_chain.run({\"question\": question})\n",
    "    print(f\"Memory state: {memory.buffer}\")\n",
    "    return response\n",
    "\n",
    "# Create a Gradio app interface\n",
    "iface = gr.Interface(\n",
    "    fn=get_important_facts,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"RAG with Llama3\",\n",
    "    description=\"Ask questions\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()  # e.g. q: In which city did the Martians arrive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430236b-3a54-480f-91b2-eaa07d44db8a",
   "metadata": {},
   "source": [
    "## Working on the interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a136cf3-aae7-4eab-a31d-b4ca0aacff7d",
   "metadata": {},
   "source": [
    "In this example, we use gradio, you will find many other interface options, and in each of them possibilities to customize the look and feel. For example in Gradio, adding Chatbot look, with option to clear the chat if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f5c662-8fb9-4c5f-b0be-3fe4e6161b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7873/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7873/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question passed to retriever: Where did the Martians land?\n",
      "Question passed to LLM: text=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nI was a lonely man, and they were very kind to me. I was a lonely man and a sad one, \\nand they bore with me. I remained with them fo ur days after my rec overy. All that time I \\nfelt a vague, a growing craving to look once more on whatever remained of the little life \\nthat seemed so happy and bright in my past . It was a mere hopeless  desire to feast upon \\nmy misery. They dissuaded me. They did all th ey could to divert me from this morbidity. \\nBut at last I could resist the impulse no l onger, and, promising faithfully to return to \\nthem, and parting, as I will confess, from thes e four-day friends with tears, I went out \\nagain into the streets that had lately been so dark and strange and empty.\\n\\nand the contorted bodies shrouded in that layer;  they rise upon me ta ttered and dog-bitten. \\nThey gibber and grow fiercer, paler, uglier, mad distortions of huma nity at last, and I \\nwake, cold and wretched, in the darkness of the night.\\n\\nAnd strangest of all is it to hold my wife 's hand again, and to think that I have counted \\nher, and that she has counted me, among the dead.\\n\\nthis planet as being fenced in and a secure abiding place for Man; we can never anticipate \\nthe unseen good or evil that may come upon us suddenly out of space. It may be that in \\nthe larger design of the universe this invasion from Mars is not  without its ultimate \\nbenefit for men; it has robbed us of that serene conf idence in the future which is the most \\nfruitful source of decadence, the gifts to human science it has brought are enormous, and \\nit has done much to promote th e conception of the commonweal of mankind. It may be \\nthat across the immensity of space the Martians have watched the fate of these pioneers of theirs and learned their lesson, and that on the planet Venus they have found a securer \\nsettlement. Be that as it may, for many year s yet there will certainly be no relaxation of \\nthe eager scrutiny of the Martian disk, and thos e fiery darts of the sky, the shooting stars, \\nwill bring with them as they fall an unavoida ble apprehension to all the sons of men.\\n\\nQuestion: Where did the Martians land?\\nHelpful Answer:\"\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: The given text is a passage from H.G. Wells' novel \"The War of the Worlds\". The story follows the narrator's account of the Martian invasion of Earth.\n",
      "\n",
      "Since there isn't any mention in this passage about where the Martians landed, I'd say:\n",
      "\n",
      "Helpful Answer: I don't know\n",
      "Memory state: [HumanMessage(content='Where did the Martians land?', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know', additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: What food did they eat?\n",
      "Question passed to LLM: text='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Where did the Martians land?\\nAssistant: The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know\\nFollow Up Input: What food did they eat?\\nStandalone question:'\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: The context is a conversation between a human and an assistant about H.G. Wells' novel \"The War of the Worlds\". The human asked where the Martians landed, but the assistant didn't provide that information. Instead, the assistant suggested that since there's no mention of it in the passage, the helpful answer would be \"I don't know\".\n",
      "\n",
      "To rephrase the follow-up question (\"What food did they eat?\") to be a standalone question, in its original language, I would say:\n",
      "\n",
      "\"What foods do the Martians eat?\"\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nHe extended a thin white hand and spoke in almost a complaining tone.  \\n   \"Why are these things permitted? What sins have we done? The morning service was \\nover, I was walking through the roads to clear my brain for the afternoon, and then--fire, \\nearthquake, death! As if it were Sodom and Gomorrah! All our work undone, all the \\nwork---- What are th ese Mar- tians?\"  \\n   \"What are we?\" I answered, clearing my throat.  \\n   He gripped his knees and turned to look at me again. For half a minute, perhaps, he \\nstared silently.     \"I was walking through the roads to cl ear my brain,\" he said. \"And suddenly--fire, \\nearthquake, death!\"  \\n   He relapsed into silence, with his chin now sunken almost to his knees.  \\n   Presently he began waving his hand.  \\n   \"All the work--all the Sunday schools-- -- What have we done--what has Weybridge \\ndone? Everything gone--e very- thing destroyed. The chur ch! We rebuilt it only three \\nyears ago. Gone! Swept out of existence! Why?\"\\n\\n\"Not begun. All that\\'s happened so far is through our not having the sense to keep \\nquiet--worrying them with guns and such foolery. And losing  our heads, and rushing off \\nin crowds to where there wasn\\'t any more sa fety than where we were. They don\\'t want to \\nbother us yet. They\\'re making their things--mak ing all the things they  couldn\\'t bring with \\nthem, getting things ready for the rest of their people. Very likely that\\'s why the cylinders \\nhave stopped for a bit, for fear of hitting t hose who are here. And in stead of our rush- ing \\nabout blind, on the howl, or ge tting dynamite on the chance of busting them up, we\\'ve got \\nto fix ourselves up according to th e new state of affairs. That\\'s how I figure it out. It isn\\'t \\nquite according to what a man wants for his spec ies, but it\\'s about what the facts point to. \\nAnd that\\'s the principle I acted upon. Cities, nations, civilisation, pr ogress--it\\'s all over. \\nThat game\\'s up. We\\'re beat.\"\\n\\nBut the trouble was the blank incongruity of this serenity and the swift death flying \\nyonder, not two miles away. There was a noise  of business from the gasworks, and the \\nelectric lamps were all alight. I stopped at the group of people.  \\n   \"What news from the common?\" said I.  \\n   There were two men and a woman at the gate.  \\n   \"Eh?\" said one of the men, turning.     \"What news from the common?\" I said.  \\n   \"\\'Ain\\'t yer just BEEN there?\" asked the men.  \\n   \"People seem fair silly about the common,\" said the woman over the gate. \"What\\'s it all \\nabart?\"  \\n   \"Haven\\'t you heard of the men from Mars ?\" said I; \"the creatures from Mars?\"  \\n   \"Quite enough,\" said the woman over the gate. \"Thenks\"; and all three of them laughed.  \\n   I felt foolish and angry. I tried and found I could not tell them what I had seen. They \\nlaughed again at my broken sentences.     \"You\\'ll hear more yet,\" I said, and went on to my home.\\n\\n(whichever way one likes to put  it) than does Mars. The invi gorating influences of this \\nexcess of oxygen upon the Martians indisputably did much to counterbalance the increased weight of their bodies. And, in the second place, we all overlooked the fact that \\nsuch mechanical intelligence as the Martian possessed was quite able to dispense with \\nmuscular exertion at a pinch.  \\n   But I did not consider these points at th e time, and so my reasoning was dead against \\nthe chances of the invaders. With wine and food, the confidence of my own table, and the \\nnecessity of reassuring my wi fe, I grew by insensible degrees courageous and secure.  \\n   \"They have done a foolish thing,\" said I,  fingering my wineglass.  \"They are dangerous \\nbecause, no doubt, they are mad with terror. Perhaps they expected to find no living \\nthings--certainly no inte lligent living things.  \\n   \"A shell in the pit\" said I, \"if the worst comes to the wors t will kill them all.\"\\n\\nQuestion: The context is a conversation between a human and an assistant about H.G. Wells\\' novel \"The War of the Worlds\". The human asked where the Martians landed, but the assistant didn\\'t provide that information. Instead, the assistant suggested that since there\\'s no mention of it in the passage, the helpful answer would be \"I don\\'t know\".\\n\\nTo rephrase the follow-up question (\"What food did they eat?\") to be a standalone question, in its original language, I would say:\\n\\n\"What foods do the Martians eat?\"\\nHelpful Answer:'\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: Since we are given a passage from H.G. Wells' novel \"The War of the Worlds\", we can assume that the conversation is about the book and not real events. Therefore, since there's no mention of the Martians' diet in the provided context, a helpful answer would be:\n",
      "\n",
      "\"I don't know.\"\n",
      "Memory state: [HumanMessage(content='Where did the Martians land?', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know', additional_kwargs={}, response_metadata={}), HumanMessage(content='What food did they eat?', additional_kwargs={}, response_metadata={}), AIMessage(content='Since we are given a passage from H.G. Wells\\' novel \"The War of the Worlds\", we can assume that the conversation is about the book and not real events. Therefore, since there\\'s no mention of the Martians\\' diet in the provided context, a helpful answer would be:\\n\\n\"I don\\'t know.\"', additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: What did the Martians machines look like?\n",
      "Question passed to LLM: text='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Where did the Martians land?\\nAssistant: The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know\\nHuman: What food did they eat?\\nAssistant: Since we are given a passage from H.G. Wells\\' novel \"The War of the Worlds\", we can assume that the conversation is about the book and not real events. Therefore, since there\\'s no mention of the Martians\\' diet in the provided context, a helpful answer would be:\\n\\n\"I don\\'t know.\"\\nFollow Up Input: What did the Martians machines look like?\\nStandalone question:'\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: Based on the conversation and follow-up question, I would rephrase the standalone question as:\n",
      "\n",
      "What did the Martian machines look like?\n",
      "\n",
      "(Note: Since there is no mention of the Martians' machines in the original passage, a helpful answer would be \"I don't know\".)\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n\"Not begun. All that\\'s happened so far is through our not having the sense to keep \\nquiet--worrying them with guns and such foolery. And losing  our heads, and rushing off \\nin crowds to where there wasn\\'t any more sa fety than where we were. They don\\'t want to \\nbother us yet. They\\'re making their things--mak ing all the things they  couldn\\'t bring with \\nthem, getting things ready for the rest of their people. Very likely that\\'s why the cylinders \\nhave stopped for a bit, for fear of hitting t hose who are here. And in stead of our rush- ing \\nabout blind, on the howl, or ge tting dynamite on the chance of busting them up, we\\'ve got \\nto fix ourselves up according to th e new state of affairs. That\\'s how I figure it out. It isn\\'t \\nquite according to what a man wants for his spec ies, but it\\'s about what the facts point to. \\nAnd that\\'s the principle I acted upon. Cities, nations, civilisation, pr ogress--it\\'s all over. \\nThat game\\'s up. We\\'re beat.\"\\n\\nBut the trouble was the blank incongruity of this serenity and the swift death flying \\nyonder, not two miles away. There was a noise  of business from the gasworks, and the \\nelectric lamps were all alight. I stopped at the group of people.  \\n   \"What news from the common?\" said I.  \\n   There were two men and a woman at the gate.  \\n   \"Eh?\" said one of the men, turning.     \"What news from the common?\" I said.  \\n   \"\\'Ain\\'t yer just BEEN there?\" asked the men.  \\n   \"People seem fair silly about the common,\" said the woman over the gate. \"What\\'s it all \\nabart?\"  \\n   \"Haven\\'t you heard of the men from Mars ?\" said I; \"the creatures from Mars?\"  \\n   \"Quite enough,\" said the woman over the gate. \"Thenks\"; and all three of them laughed.  \\n   I felt foolish and angry. I tried and found I could not tell them what I had seen. They \\nlaughed again at my broken sentences.     \"You\\'ll hear more yet,\" I said, and went on to my home.\\n\\nHe extended a thin white hand and spoke in almost a complaining tone.  \\n   \"Why are these things permitted? What sins have we done? The morning service was \\nover, I was walking through the roads to clear my brain for the afternoon, and then--fire, \\nearthquake, death! As if it were Sodom and Gomorrah! All our work undone, all the \\nwork---- What are th ese Mar- tians?\"  \\n   \"What are we?\" I answered, clearing my throat.  \\n   He gripped his knees and turned to look at me again. For half a minute, perhaps, he \\nstared silently.     \"I was walking through the roads to cl ear my brain,\" he said. \"And suddenly--fire, \\nearthquake, death!\"  \\n   He relapsed into silence, with his chin now sunken almost to his knees.  \\n   Presently he began waving his hand.  \\n   \"All the work--all the Sunday schools-- -- What have we done--what has Weybridge \\ndone? Everything gone--e very- thing destroyed. The chur ch! We rebuilt it only three \\nyears ago. Gone! Swept out of existence! Why?\"\\n\\nI was a lonely man, and they were very kind to me. I was a lonely man and a sad one, \\nand they bore with me. I remained with them fo ur days after my rec overy. All that time I \\nfelt a vague, a growing craving to look once more on whatever remained of the little life \\nthat seemed so happy and bright in my past . It was a mere hopeless  desire to feast upon \\nmy misery. They dissuaded me. They did all th ey could to divert me from this morbidity. \\nBut at last I could resist the impulse no l onger, and, promising faithfully to return to \\nthem, and parting, as I will confess, from thes e four-day friends with tears, I went out \\nagain into the streets that had lately been so dark and strange and empty.\\n\\nQuestion: Based on the conversation and follow-up question, I would rephrase the standalone question as:\\n\\nWhat did the Martian machines look like?\\n\\n(Note: Since there is no mention of the Martians\\' machines in the original passage, a helpful answer would be \"I don\\'t know\".)\\nHelpful Answer:'\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: The given text appears to be an excerpt from H.G. Wells' classic science fiction novel \"The War of the Worlds\". The passage describes the events following the invasion of Earth by Martians, and the protagonist's struggle to come to terms with the devastation.\n",
      "\n",
      "In this context, the question being asked is what did the Martian machines look like?\n",
      "Memory state: [HumanMessage(content='Where did the Martians land?', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know', additional_kwargs={}, response_metadata={}), HumanMessage(content='What food did they eat?', additional_kwargs={}, response_metadata={}), AIMessage(content='Since we are given a passage from H.G. Wells\\' novel \"The War of the Worlds\", we can assume that the conversation is about the book and not real events. Therefore, since there\\'s no mention of the Martians\\' diet in the provided context, a helpful answer would be:\\n\\n\"I don\\'t know.\"', additional_kwargs={}, response_metadata={}), HumanMessage(content='What did the Martians machines look like?', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text appears to be an excerpt from H.G. Wells\\' classic science fiction novel \"The War of the Worlds\". The passage describes the events following the invasion of Earth by Martians, and the protagonist\\'s struggle to come to terms with the devastation.\\n\\nIn this context, the question being asked is what did the Martian machines look like?', additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: What did the Martians machines look like?\n",
      "Question passed to LLM: text='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Where did the Martians land?\\nAssistant: The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know\\nHuman: What food did they eat?\\nAssistant: Since we are given a passage from H.G. Wells\\' novel \"The War of the Worlds\", we can assume that the conversation is about the book and not real events. Therefore, since there\\'s no mention of the Martians\\' diet in the provided context, a helpful answer would be:\\n\\n\"I don\\'t know.\"\\nHuman: What did the Martians machines look like?\\nAssistant: The given text appears to be an excerpt from H.G. Wells\\' classic science fiction novel \"The War of the Worlds\". The passage describes the events following the invasion of Earth by Martians, and the protagonist\\'s struggle to come to terms with the devastation.\\n\\nIn this context, the question being asked is what did the Martian machines look like?\\nFollow Up Input: What did the Martians machines look like?\\nStandalone question:'\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: What did the Martians' machines look like?\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nI was a lonely man, and they were very kind to me. I was a lonely man and a sad one, \\nand they bore with me. I remained with them fo ur days after my rec overy. All that time I \\nfelt a vague, a growing craving to look once more on whatever remained of the little life \\nthat seemed so happy and bright in my past . It was a mere hopeless  desire to feast upon \\nmy misery. They dissuaded me. They did all th ey could to divert me from this morbidity. \\nBut at last I could resist the impulse no l onger, and, promising faithfully to return to \\nthem, and parting, as I will confess, from thes e four-day friends with tears, I went out \\nagain into the streets that had lately been so dark and strange and empty.\\n\\nthis planet as being fenced in and a secure abiding place for Man; we can never anticipate \\nthe unseen good or evil that may come upon us suddenly out of space. It may be that in \\nthe larger design of the universe this invasion from Mars is not  without its ultimate \\nbenefit for men; it has robbed us of that serene conf idence in the future which is the most \\nfruitful source of decadence, the gifts to human science it has brought are enormous, and \\nit has done much to promote th e conception of the commonweal of mankind. It may be \\nthat across the immensity of space the Martians have watched the fate of these pioneers of theirs and learned their lesson, and that on the planet Venus they have found a securer \\nsettlement. Be that as it may, for many year s yet there will certainly be no relaxation of \\nthe eager scrutiny of the Martian disk, and thos e fiery darts of the sky, the shooting stars, \\nwill bring with them as they fall an unavoida ble apprehension to all the sons of men.\\n\\n\"Not begun. All that\\'s happened so far is through our not having the sense to keep \\nquiet--worrying them with guns and such foolery. And losing  our heads, and rushing off \\nin crowds to where there wasn\\'t any more sa fety than where we were. They don\\'t want to \\nbother us yet. They\\'re making their things--mak ing all the things they  couldn\\'t bring with \\nthem, getting things ready for the rest of their people. Very likely that\\'s why the cylinders \\nhave stopped for a bit, for fear of hitting t hose who are here. And in stead of our rush- ing \\nabout blind, on the howl, or ge tting dynamite on the chance of busting them up, we\\'ve got \\nto fix ourselves up according to th e new state of affairs. That\\'s how I figure it out. It isn\\'t \\nquite according to what a man wants for his spec ies, but it\\'s about what the facts point to. \\nAnd that\\'s the principle I acted upon. Cities, nations, civilisation, pr ogress--it\\'s all over. \\nThat game\\'s up. We\\'re beat.\"\\n\\nAnd strangest of all is it to hold my wife \\'s hand again, and to think that I have counted \\nher, and that she has counted me, among the dead.\\n\\nQuestion: What did the Martians\\' machines look like?\\nHelpful Answer:'\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: It seems you want me to help answer a question based on some text. The question is: \"What did the Martians' machines look like?\" and I'm supposed to provide an answer that's \"helpful\" according to the given context.\n",
      "\n",
      "Unfortunately, there isn't any description of the Martians' machines in the provided text. It seems to be about the narrator's thoughts on the invasion from Mars, his interactions with humans who survived the initial attack, and how he has come to terms with the fact that humanity is \"beat\".\n",
      "\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer. That's exactly what I'm going to do in this case - since there is no description of the Martians' machines in the text, I can't provide a helpful answer.\n",
      "\n",
      "So, my answer would be: \"I don't know.\"\n",
      "Memory state: [HumanMessage(content='Where did the Martians land?', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know', additional_kwargs={}, response_metadata={}), HumanMessage(content='What food did they eat?', additional_kwargs={}, response_metadata={}), AIMessage(content='Since we are given a passage from H.G. Wells\\' novel \"The War of the Worlds\", we can assume that the conversation is about the book and not real events. Therefore, since there\\'s no mention of the Martians\\' diet in the provided context, a helpful answer would be:\\n\\n\"I don\\'t know.\"', additional_kwargs={}, response_metadata={}), HumanMessage(content='What did the Martians machines look like?', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text appears to be an excerpt from H.G. Wells\\' classic science fiction novel \"The War of the Worlds\". The passage describes the events following the invasion of Earth by Martians, and the protagonist\\'s struggle to come to terms with the devastation.\\n\\nIn this context, the question being asked is what did the Martian machines look like?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What did the Martians machines look like?', additional_kwargs={}, response_metadata={}), AIMessage(content='It seems you want me to help answer a question based on some text. The question is: \"What did the Martians\\' machines look like?\" and I\\'m supposed to provide an answer that\\'s \"helpful\" according to the given context.\\n\\nUnfortunately, there isn\\'t any description of the Martians\\' machines in the provided text. It seems to be about the narrator\\'s thoughts on the invasion from Mars, his interactions with humans who survived the initial attack, and how he has come to terms with the fact that humanity is \"beat\".\\n\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer. That\\'s exactly what I\\'m going to do in this case - since there is no description of the Martians\\' machines in the text, I can\\'t provide a helpful answer.\\n\\nSo, my answer would be: \"I don\\'t know.\"', additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: Tell me about the MArtian rockets\n",
      "Question passed to LLM: text='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Where did the Martians land?\\nAssistant: The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know\\nHuman: What food did they eat?\\nAssistant: Since we are given a passage from H.G. Wells\\' novel \"The War of the Worlds\", we can assume that the conversation is about the book and not real events. Therefore, since there\\'s no mention of the Martians\\' diet in the provided context, a helpful answer would be:\\n\\n\"I don\\'t know.\"\\nHuman: What did the Martians machines look like?\\nAssistant: The given text appears to be an excerpt from H.G. Wells\\' classic science fiction novel \"The War of the Worlds\". The passage describes the events following the invasion of Earth by Martians, and the protagonist\\'s struggle to come to terms with the devastation.\\n\\nIn this context, the question being asked is what did the Martian machines look like?\\nHuman: What did the Martians machines look like?\\nAssistant: It seems you want me to help answer a question based on some text. The question is: \"What did the Martians\\' machines look like?\" and I\\'m supposed to provide an answer that\\'s \"helpful\" according to the given context.\\n\\nUnfortunately, there isn\\'t any description of the Martians\\' machines in the provided text. It seems to be about the narrator\\'s thoughts on the invasion from Mars, his interactions with humans who survived the initial attack, and how he has come to terms with the fact that humanity is \"beat\".\\n\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer. That\\'s exactly what I\\'m going to do in this case - since there is no description of the Martians\\' machines in the text, I can\\'t provide a helpful answer.\\n\\nSo, my answer would be: \"I don\\'t know.\"\\nFollow Up Input: Tell me about the MArtian rockets\\nStandalone question:'\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: What did the Martian rockets look like?\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nI was a lonely man, and they were very kind to me. I was a lonely man and a sad one, \\nand they bore with me. I remained with them fo ur days after my rec overy. All that time I \\nfelt a vague, a growing craving to look once more on whatever remained of the little life \\nthat seemed so happy and bright in my past . It was a mere hopeless  desire to feast upon \\nmy misery. They dissuaded me. They did all th ey could to divert me from this morbidity. \\nBut at last I could resist the impulse no l onger, and, promising faithfully to return to \\nthem, and parting, as I will confess, from thes e four-day friends with tears, I went out \\nagain into the streets that had lately been so dark and strange and empty.\\n\\nthis planet as being fenced in and a secure abiding place for Man; we can never anticipate \\nthe unseen good or evil that may come upon us suddenly out of space. It may be that in \\nthe larger design of the universe this invasion from Mars is not  without its ultimate \\nbenefit for men; it has robbed us of that serene conf idence in the future which is the most \\nfruitful source of decadence, the gifts to human science it has brought are enormous, and \\nit has done much to promote th e conception of the commonweal of mankind. It may be \\nthat across the immensity of space the Martians have watched the fate of these pioneers of theirs and learned their lesson, and that on the planet Venus they have found a securer \\nsettlement. Be that as it may, for many year s yet there will certainly be no relaxation of \\nthe eager scrutiny of the Martian disk, and thos e fiery darts of the sky, the shooting stars, \\nwill bring with them as they fall an unavoida ble apprehension to all the sons of men.\\n\\nand the contorted bodies shrouded in that layer;  they rise upon me ta ttered and dog-bitten. \\nThey gibber and grow fiercer, paler, uglier, mad distortions of huma nity at last, and I \\nwake, cold and wretched, in the darkness of the night.\\n\\nBut the trouble was the blank incongruity of this serenity and the swift death flying \\nyonder, not two miles away. There was a noise  of business from the gasworks, and the \\nelectric lamps were all alight. I stopped at the group of people.  \\n   \"What news from the common?\" said I.  \\n   There were two men and a woman at the gate.  \\n   \"Eh?\" said one of the men, turning.     \"What news from the common?\" I said.  \\n   \"\\'Ain\\'t yer just BEEN there?\" asked the men.  \\n   \"People seem fair silly about the common,\" said the woman over the gate. \"What\\'s it all \\nabart?\"  \\n   \"Haven\\'t you heard of the men from Mars ?\" said I; \"the creatures from Mars?\"  \\n   \"Quite enough,\" said the woman over the gate. \"Thenks\"; and all three of them laughed.  \\n   I felt foolish and angry. I tried and found I could not tell them what I had seen. They \\nlaughed again at my broken sentences.     \"You\\'ll hear more yet,\" I said, and went on to my home.\\n\\nQuestion: What did the Martian rockets look like?\\nHelpful Answer:'\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: The given text is from a novel by H.G. Wells, titled \"The War of the Worlds\". The context includes descriptions of the narrator's emotional state after recovering from an illness and his subsequent departure from four-day friends to revisit his past life. The narrative then shifts to a discussion about the Martian invasion, which has brought enormous scientific benefits but also led to humanity's loss of serene confidence in the future.\n",
      "\n",
      "Please note that there is no description of the Martian rockets in this text. If you are looking for an answer based on this context, it would be \"I don't know\" because the text does not provide any information about the appearance or characteristics of the Martian rockets.\n",
      "Memory state: [HumanMessage(content='Where did the Martians land?', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know', additional_kwargs={}, response_metadata={}), HumanMessage(content='What food did they eat?', additional_kwargs={}, response_metadata={}), AIMessage(content='Since we are given a passage from H.G. Wells\\' novel \"The War of the Worlds\", we can assume that the conversation is about the book and not real events. Therefore, since there\\'s no mention of the Martians\\' diet in the provided context, a helpful answer would be:\\n\\n\"I don\\'t know.\"', additional_kwargs={}, response_metadata={}), HumanMessage(content='What did the Martians machines look like?', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text appears to be an excerpt from H.G. Wells\\' classic science fiction novel \"The War of the Worlds\". The passage describes the events following the invasion of Earth by Martians, and the protagonist\\'s struggle to come to terms with the devastation.\\n\\nIn this context, the question being asked is what did the Martian machines look like?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What did the Martians machines look like?', additional_kwargs={}, response_metadata={}), AIMessage(content='It seems you want me to help answer a question based on some text. The question is: \"What did the Martians\\' machines look like?\" and I\\'m supposed to provide an answer that\\'s \"helpful\" according to the given context.\\n\\nUnfortunately, there isn\\'t any description of the Martians\\' machines in the provided text. It seems to be about the narrator\\'s thoughts on the invasion from Mars, his interactions with humans who survived the initial attack, and how he has come to terms with the fact that humanity is \"beat\".\\n\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer. That\\'s exactly what I\\'m going to do in this case - since there is no description of the Martians\\' machines in the text, I can\\'t provide a helpful answer.\\n\\nSo, my answer would be: \"I don\\'t know.\"', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about the MArtian rockets', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text is from a novel by H.G. Wells, titled \"The War of the Worlds\". The context includes descriptions of the narrator\\'s emotional state after recovering from an illness and his subsequent departure from four-day friends to revisit his past life. The narrative then shifts to a discussion about the Martian invasion, which has brought enormous scientific benefits but also led to humanity\\'s loss of serene confidence in the future.\\n\\nPlease note that there is no description of the Martian rockets in this text. If you are looking for an answer based on this context, it would be \"I don\\'t know\" because the text does not provide any information about the appearance or characteristics of the Martian rockets.', additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: Tell me about the MArtian invasion\n",
      "Question passed to LLM: text='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Where did the Martians land?\\nAssistant: The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know\\nHuman: What food did they eat?\\nAssistant: Since we are given a passage from H.G. Wells\\' novel \"The War of the Worlds\", we can assume that the conversation is about the book and not real events. Therefore, since there\\'s no mention of the Martians\\' diet in the provided context, a helpful answer would be:\\n\\n\"I don\\'t know.\"\\nHuman: What did the Martians machines look like?\\nAssistant: The given text appears to be an excerpt from H.G. Wells\\' classic science fiction novel \"The War of the Worlds\". The passage describes the events following the invasion of Earth by Martians, and the protagonist\\'s struggle to come to terms with the devastation.\\n\\nIn this context, the question being asked is what did the Martian machines look like?\\nHuman: What did the Martians machines look like?\\nAssistant: It seems you want me to help answer a question based on some text. The question is: \"What did the Martians\\' machines look like?\" and I\\'m supposed to provide an answer that\\'s \"helpful\" according to the given context.\\n\\nUnfortunately, there isn\\'t any description of the Martians\\' machines in the provided text. It seems to be about the narrator\\'s thoughts on the invasion from Mars, his interactions with humans who survived the initial attack, and how he has come to terms with the fact that humanity is \"beat\".\\n\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer. That\\'s exactly what I\\'m going to do in this case - since there is no description of the Martians\\' machines in the text, I can\\'t provide a helpful answer.\\n\\nSo, my answer would be: \"I don\\'t know.\"\\nHuman: Tell me about the MArtian rockets\\nAssistant: The given text is from a novel by H.G. Wells, titled \"The War of the Worlds\". The context includes descriptions of the narrator\\'s emotional state after recovering from an illness and his subsequent departure from four-day friends to revisit his past life. The narrative then shifts to a discussion about the Martian invasion, which has brought enormous scientific benefits but also led to humanity\\'s loss of serene confidence in the future.\\n\\nPlease note that there is no description of the Martian rockets in this text. If you are looking for an answer based on this context, it would be \"I don\\'t know\" because the text does not provide any information about the appearance or characteristics of the Martian rockets.\\nFollow Up Input: Tell me about the MArtian invasion\\nStandalone question:'\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: The standalone follow-up question in its original language is:\n",
      "\n",
      "Tell me about the Martian invasion.\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nAnd strangest of all is it to hold my wife \\'s hand again, and to think that I have counted \\nher, and that she has counted me, among the dead.\\n\\nthis planet as being fenced in and a secure abiding place for Man; we can never anticipate \\nthe unseen good or evil that may come upon us suddenly out of space. It may be that in \\nthe larger design of the universe this invasion from Mars is not  without its ultimate \\nbenefit for men; it has robbed us of that serene conf idence in the future which is the most \\nfruitful source of decadence, the gifts to human science it has brought are enormous, and \\nit has done much to promote th e conception of the commonweal of mankind. It may be \\nthat across the immensity of space the Martians have watched the fate of these pioneers of theirs and learned their lesson, and that on the planet Venus they have found a securer \\nsettlement. Be that as it may, for many year s yet there will certainly be no relaxation of \\nthe eager scrutiny of the Martian disk, and thos e fiery darts of the sky, the shooting stars, \\nwill bring with them as they fall an unavoida ble apprehension to all the sons of men.\\n\\nand the contorted bodies shrouded in that layer;  they rise upon me ta ttered and dog-bitten. \\nThey gibber and grow fiercer, paler, uglier, mad distortions of huma nity at last, and I \\nwake, cold and wretched, in the darkness of the night.\\n\\npast four I went up to the railway station to get an even ing paper, for the morning papers \\nhad contained only a very inaccurate descri ption of the killing of Stent, Henderson, \\nOgilvy, and the others. But there was little I didn\\'t know. The Martia ns did not show an \\ninch of themselves. They seemed busy in their pit, and there was a sound of hammering \\nand an almost continuous streamer of smoke . Apparently they were busy getting ready \\nfor a struggle. \"Fresh attempts have been ma de to signal, but without success,\" was the \\nstereo- typed formula of the papers. A sappe r told me it was done by a man in a ditch \\nwith a flag on a long pole. The Martians took as much notice of such advances as we \\nshould of the lowing of a cow.\\n\\nQuestion: The standalone follow-up question in its original language is:\\n\\nTell me about the Martian invasion.\\nHelpful Answer:'\n",
      "Context passed to LLM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: The context is a passage from H.G. Wells' science fiction novel \"The War of the Worlds\", specifically chapters 1 and 2. The passage describes the narrator's reactions to the Martian invasion, including his feelings about holding his wife's hand again after initially thinking she was dead, and his observations about the Martians' activities on Earth.\n",
      "Memory state: [HumanMessage(content='Where did the Martians land?', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text is a passage from H.G. Wells\\' novel \"The War of the Worlds\". The story follows the narrator\\'s account of the Martian invasion of Earth.\\n\\nSince there isn\\'t any mention in this passage about where the Martians landed, I\\'d say:\\n\\nHelpful Answer: I don\\'t know', additional_kwargs={}, response_metadata={}), HumanMessage(content='What food did they eat?', additional_kwargs={}, response_metadata={}), AIMessage(content='Since we are given a passage from H.G. Wells\\' novel \"The War of the Worlds\", we can assume that the conversation is about the book and not real events. Therefore, since there\\'s no mention of the Martians\\' diet in the provided context, a helpful answer would be:\\n\\n\"I don\\'t know.\"', additional_kwargs={}, response_metadata={}), HumanMessage(content='What did the Martians machines look like?', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text appears to be an excerpt from H.G. Wells\\' classic science fiction novel \"The War of the Worlds\". The passage describes the events following the invasion of Earth by Martians, and the protagonist\\'s struggle to come to terms with the devastation.\\n\\nIn this context, the question being asked is what did the Martian machines look like?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What did the Martians machines look like?', additional_kwargs={}, response_metadata={}), AIMessage(content='It seems you want me to help answer a question based on some text. The question is: \"What did the Martians\\' machines look like?\" and I\\'m supposed to provide an answer that\\'s \"helpful\" according to the given context.\\n\\nUnfortunately, there isn\\'t any description of the Martians\\' machines in the provided text. It seems to be about the narrator\\'s thoughts on the invasion from Mars, his interactions with humans who survived the initial attack, and how he has come to terms with the fact that humanity is \"beat\".\\n\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer. That\\'s exactly what I\\'m going to do in this case - since there is no description of the Martians\\' machines in the text, I can\\'t provide a helpful answer.\\n\\nSo, my answer would be: \"I don\\'t know.\"', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about the MArtian rockets', additional_kwargs={}, response_metadata={}), AIMessage(content='The given text is from a novel by H.G. Wells, titled \"The War of the Worlds\". The context includes descriptions of the narrator\\'s emotional state after recovering from an illness and his subsequent departure from four-day friends to revisit his past life. The narrative then shifts to a discussion about the Martian invasion, which has brought enormous scientific benefits but also led to humanity\\'s loss of serene confidence in the future.\\n\\nPlease note that there is no description of the Martian rockets in this text. If you are looking for an answer based on this context, it would be \"I don\\'t know\" because the text does not provide any information about the appearance or characteristics of the Martian rockets.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about the MArtian invasion', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage from H.G. Wells\\' science fiction novel \"The War of the Worlds\", specifically chapters 1 and 2. The passage describes the narrator\\'s reactions to the Martian invasion, including his feelings about holding his wife\\'s hand again after initially thinking she was dead, and his observations about the Martians\\' activities on Earth.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define memory to store the previous exchanges\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Custom Runnable LLM class with debugging\n",
    "class OllamaLLM(Runnable):\n",
    "    def __init__(self, llm_fn):\n",
    "        self.llm_fn = llm_fn\n",
    "\n",
    "    def invoke(self, input, config=None, **kwargs):\n",
    "        question = str(input)  # Convert the input to a string if necessary\n",
    "        context = kwargs.get(\"context\", \"\")  # Retrieve context from kwargs if available\n",
    "\n",
    "        # Debugging outputs\n",
    "        print(f\"Question passed to LLM: {question}\")\n",
    "        print(f\"Context passed to LLM: {context}\")\n",
    "\n",
    "        stop = kwargs.get(\"stop\", None)\n",
    "        response = self.llm_fn(question, context)\n",
    "        \n",
    "        # Debugging output\n",
    "        print(f\"Response from LLM: {response}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    def predict(self, input, **kwargs):\n",
    "        return self.invoke(input, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.invoke(*args, **kwargs)\n",
    "\n",
    "# Instantiate the custom LLM class\n",
    "ollama_llm_instance = OllamaLLM(ollama_llm)\n",
    "\n",
    "# Define the conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ollama_llm_instance,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Define the function to get important facts with debugging\n",
    "def get_important_facts(question, chat_history):\n",
    "    print(f\"Question passed to retriever: {question}\")\n",
    "    \n",
    "    response = qa_chain.run({\"question\": question})\n",
    "    print(f\"Memory state: {memory.buffer}\")\n",
    "    \n",
    "    # Append to chat history\n",
    "    chat_history.append((question, response))\n",
    "    return chat_history, chat_history\n",
    "\n",
    "# Create a Gradio chat interface\n",
    "with gr.Blocks() as iface:\n",
    "    gr.Markdown(\"# RAG with Llama3\")\n",
    "    chatbot = gr.Chatbot(height=250)  # Adjust the height here\n",
    "    question_input = gr.Textbox(lines=2, placeholder=\"Enter your question here...\", show_label=False)\n",
    "    submit_btn = gr.Button(\"Submit\")\n",
    "    clear_btn = gr.Button(\"Clear Chat\")\n",
    "    \n",
    "    # Set up the interaction\n",
    "    submit_btn.click(get_important_facts, [question_input, chatbot], [chatbot, chatbot])\n",
    "    question_input.submit(get_important_facts, [question_input, chatbot], [chatbot, chatbot])\n",
    "    clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n",
    "# e.g. q: Tell me about the Martian invasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0510a17-cebd-4feb-8fc6-d4b80a48fb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb27f3-b0e1-474e-b25c-52e82024058b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30071c-fd90-434f-b275-e55f69e58feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3f8e9-9954-4ac5-8db6-2c486e904a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e1765-f7f5-4041-adaa-61f68336f867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fc608-1a86-4ef4-a129-eed1d4956046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
