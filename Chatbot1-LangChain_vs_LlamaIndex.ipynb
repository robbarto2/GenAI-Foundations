{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "18d42395-42d0-4bc7-ab50-b66c75e43834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain\n",
    "#!pip install -U langchain-community\n",
    "\n",
    "#!pip install llama-index pypdf\n",
    "#!pip install llama-index --upgrade\n",
    "\n",
    "# Don't bother me with warnings\n",
    "import warnings # optional, disabling warnings about versions and others\n",
    "warnings.filterwarnings('ignore') # optional, disabling warnings about versions and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e0a1259b-d564-4ad5-adde-461725da952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   The Martians seem to have calculated their descent with amazing subtlety--their \n",
      "mathematical learning is evidently far in exce ss of ours--and to ha ve carried out their \n",
      "prepara- tions with a well-nigh perfect unanimity. Had our instru- ments permitted it, we might have seen the gathering trouble far back in the nineteenth century. Men like \n",
      "Schiaparelli watched the red planet--it is odd, by-the-bye, that for count- less centuries \n",
      "Mars has been the star of war--but failed to interpret the flu\n"
     ]
    }
   ],
   "source": [
    "# Loading a simple PDF with Langchain, straightforward\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/War-of-the-Worlds.pdf\")\n",
    "book = loader.load()\n",
    "#Looking at a small extract, one page, and a few hundred characters in that page\n",
    "page = book[3]\n",
    "print(page.page_content[1660:2164])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "73b7b92d-da0f-43ed-85e4-7e1e7bffb084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Martians seem to have calculated their descent with amazing subtlety--their \n",
      "mathematical learning is evidently far in exce ss of ours--and to ha ve carried out their \n",
      "prepara- tions with a well-nigh perfect unanimity. Had our instru- ments permitted it, we might have seen the gathering trouble far back in the nineteenth century. Men like \n",
      "Schiaparelli watched the red planet--it is odd, by-the-bye, that for count- less centuries \n",
      "Mars has been the star of war--but failed to interpret the flu\n"
     ]
    }
   ],
   "source": [
    "# Loading a simple docuemnt with LlamaIndex, also straightforward\n",
    "\n",
    "from llama_index.core import GPTVectorStoreIndex, Document\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Load a specific PDF file\n",
    "pdf_path = \"docs/War-of-the-Worlds.pdf\"\n",
    "\n",
    "# Use PyPDF to extract the text\n",
    "reader = PdfReader(pdf_path)\n",
    "pdf_text = \"\"\n",
    "for page in reader.pages:\n",
    "    pdf_text += page.extract_text()\n",
    "\n",
    "# Create a LlamaIndex Document object from the extracted text\n",
    "document = Document(text=pdf_text)\n",
    "\n",
    "# Check the content of the document\n",
    "print(document.get_text()[8444:8944])  # Print 500 characters to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a7b13084-d46b-46d4-aa78-772e32a7b9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=2vkJ7v0x-Fs\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading webpage\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading ios player API JSON\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading web creator player API JSON\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading m3u8 information\n",
      "[info] 2vkJ7v0x-Fs: Downloading 1 format(s): 251\n",
      "[download] Destination: docs/youtube/Big Data Architectures.webm\n",
      "[download] 100% of   22.03MiB in 00:00:01 at 11.07MiB/s    \n",
      "[ExtractAudio] Destination: docs/youtube/Big Data Architectures.m4a\n",
      "Deleting original file docs/youtube/Big Data Architectures.webm (pass -k to keep)\n",
      " In lesson four, we will go deeper into architectures for big data, and we will take a closer look at some of the most popular big data management systems. First, we're going to look at how the big data management system framework looks, and explore the commonalities that pretty much all the big data systems have, as well as some of the key differences between no SQL, MPP, and Hadoop. Next, we're going to take a deep dive into the Hadoop data management system. You will see how we both store data in HDFS, the database, as well as how data analytics processing works with MapReduce. We will also give some examples, and you will see how the Apache ecosystem of projects helps to expand and connect Hadoop with other tools, including machine learning libraries and other data management systems. Finally, we will look at the evolution of Hadoop based on Yarn. Yarn is a quantum leap forward for Hadoop that allows it to support almost any type of deep analytics and machine learning framework. In\n"
     ]
    }
   ],
   "source": [
    "# loading a video file and saving the audio to a text file, with LangChain\n",
    "\n",
    "import os\n",
    "import whisper\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# Step 1: Set up the download options\n",
    "url = \"https://www.youtube.com/watch?v=2vkJ7v0x-Fs\"\n",
    "save_dir = \"docs/youtube/\"\n",
    "output_template = os.path.join(save_dir, '%(title)s.%(ext)s')\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': output_template,  # Save the file to the specified directory with a title-based name\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'm4a',  # You can change this to mp3 if you prefer\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'ffmpeg_location': '/opt/homebrew/bin/ffmpeg',  # Specify the location of ffmpeg\n",
    "}\n",
    "\n",
    "# Step 2: Download the audio from the YouTube video\n",
    "with YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])\n",
    "\n",
    "# Step 3: Find the downloaded file\n",
    "downloaded_file = [f for f in os.listdir(save_dir) if f.endswith('.m4a')][0]  # Assuming m4a, adjust if using mp3\n",
    "downloaded_file_path = os.path.join(save_dir, downloaded_file)\n",
    "\n",
    "# Step 4: Load the Whisper model and transcribe the audio file\n",
    "model = whisper.load_model(\"base\")  # You can choose 'tiny', 'base', 'small', 'medium', or 'large'\n",
    "result = model.transcribe(downloaded_file_path)\n",
    "\n",
    "# Step 5: Adding metadata to the transcript, and saving the transcript to a file so we can use it outside of this program.\n",
    "class Document:\n",
    "    def __init__(self, source, text, metadata=None):\n",
    "        self.source = source\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "# Step 6: Wrap the transcription result in the Document class with metadata\n",
    "document = Document(\n",
    "    source=downloaded_file_path,\n",
    "    text=result['text'], \n",
    "    metadata={\"source\": \"youtube\", \"file_path\": downloaded_file_path}\n",
    ")\n",
    "#Step 7: Save the transcript to a text file\n",
    "transcript_file_path = os.path.join(save_dir, 'transcript_w_Langchain.txt')\n",
    "with open(transcript_file_path, 'w') as f:\n",
    "    f.write(result['text'])\n",
    "\n",
    "# Step 8: Print the first 1000 characters of the transcript\n",
    "print(document.page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1b5fff9f-93fb-4363-b43e-39b543e6b7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=2vkJ7v0x-Fs\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading webpage\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading ios player API JSON\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading web creator player API JSON\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading m3u8 information\n",
      "[info] 2vkJ7v0x-Fs: Downloading 1 format(s): 251\n",
      "[download] Destination: docs/youtube/Big Data Architectures.webm\n",
      "[download] 100% of   22.03MiB in 00:00:01 at 11.51MiB/s    \n",
      "[ExtractAudio] Destination: docs/youtube/Big Data Architectures.m4a\n",
      "Deleting original file docs/youtube/Big Data Architectures.webm (pass -k to keep)\n",
      "Transcript saved to docs/youtube/transcript_w_LlamaIndex.txt\n",
      "Shortened Response:  In lesson four, we will go deeper into architectures for big data, and we will take a closer look at some of the most popular big data management systems. First, we're going to look at how the big data management system framework looks, and explore the commonalities that pretty much all the big data systems have, as well as some of the key differences between no SQL, MPP, and Hadoop. Next, we're going to take a deep dive into the Hadoop data management system. You will see how we both store data in HDFS, the database, as well as how data analytics processing works with MapReduce. We will also give some examples, and you will see how the Apache ecosystem of projects helps to expand and connect Hadoop with other tools, including machine learning libraries and other data management systems. Finally, we will look at the evolution of Hadoop based on Yarn. Yarn is a quantum leap forward for Hadoop that allows it to support almost any type of deep analytics and machine learning framework. In\n"
     ]
    }
   ],
   "source": [
    "# loading a video file and saving the audio to a text file, with LlamaIndex\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import whisper\n",
    "from yt_dlp import YoutubeDL\n",
    "from llama_index.core import GPTVectorStoreIndex, Document as LlamaDocument\n",
    "from llama_index.core.base.embeddings.base import BaseEmbedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pydantic import Field\n",
    "\n",
    "# Step 1: setup the option to download and transcribe YouTube video (same as before)\n",
    "url = \"https://www.youtube.com/watch?v=2vkJ7v0x-Fs\"\n",
    "save_dir = \"docs/youtube/\"\n",
    "output_template = os.path.join(save_dir, '%(title)s.%(ext)s')\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': output_template,\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'm4a',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'ffmpeg_location': '/opt/homebrew/bin/ffmpeg',\n",
    "}\n",
    "\n",
    "# Step 2: Download the audio\n",
    "with YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])\n",
    "\n",
    "# Step 3: Find the downloaded file\n",
    "downloaded_file = [f for f in os.listdir(save_dir) if f.endswith('.m4a')][0]\n",
    "downloaded_file_path = os.path.join(save_dir, downloaded_file)\n",
    "\n",
    "# Step 4: Transcribe the audio file using Whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(downloaded_file_path)\n",
    "transcribed_text = result['text']\n",
    "\n",
    "# Step 5: Use Hugging Face SentenceTransformer for embedding\n",
    "hf_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Custom embedding class using Hugging Face model\n",
    "class HuggingFaceEmbedding(BaseEmbedding):\n",
    "    hf_model: SentenceTransformer = Field(default=None)\n",
    "\n",
    "    def __init__(self, hf_model):\n",
    "        super().__init__()\n",
    "        self.hf_model = hf_model\n",
    "\n",
    "    def _get_text_embedding(self, text):\n",
    "        # Return embedding for a single string of text\n",
    "        return self.hf_model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "    def _get_text_embeddings(self, texts):\n",
    "        # Return embeddings for a list of strings\n",
    "        return self.hf_model.encode(texts, convert_to_numpy=True)\n",
    "    \n",
    "    def _get_query_embedding(self, query):\n",
    "        # Return embedding for a query\n",
    "        return self._get_text_embedding(query)\n",
    "    \n",
    "    async def _aget_query_embedding(self, query):\n",
    "        # Async version of embedding for a query\n",
    "        return self._get_text_embedding(query)\n",
    "\n",
    "# Adding metadata to the transcript, and saving the transcript to a file\n",
    "class Document:\n",
    "    def __init__(self, source, text, metadata=None):\n",
    "        self.source = source\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "# Step 6: Wrap the transcription result in the Document class with metadata\n",
    "document = Document(\n",
    "    source=downloaded_file_path,\n",
    "    text=transcribed_text, \n",
    "    metadata={\"source\": \"youtube\", \"file_path\": downloaded_file_path}\n",
    ")\n",
    "\n",
    "# Step 7: Save the transcript to a text file\n",
    "transcript_file_path = os.path.join(save_dir, 'transcript_w_LlamaIndex.txt')\n",
    "with open(transcript_file_path, 'w') as f:\n",
    "    f.write(transcribed_text)\n",
    "\n",
    "print(f\"Transcript saved to {transcript_file_path}\")\n",
    "\n",
    "# Step 8: Use the LlamaIndex embedding model with the Hugging Face embedding model to retrieve and print the text\n",
    "llama_document = LlamaDocument(text=transcribed_text)\n",
    "embed_model = HuggingFaceEmbedding(hf_model)\n",
    "index = GPTVectorStoreIndex([llama_document], embed_model=embed_model)\n",
    "\n",
    "# Directly use the retriever (no LLM required)\n",
    "retriever = index.as_retriever()\n",
    "\n",
    "# Perform a query using the retriever\n",
    "response = retriever.retrieve(\"What is the video about?\") # this could be any question, as all we do below is retrieve the first 1000 characters of the transcript\n",
    "\n",
    "# Print the first 1000 characters of the response text\n",
    "if response:\n",
    "    shortened_response = response[0].node.text[:1000]  # Get the text from the node and limit to 500 characters\n",
    "    print(f\"Shortened Response: {shortened_response}\")\n",
    "else:\n",
    "    print(\"No response retrieved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7003d31-3013-4f95-9bf2-b96195e7990c",
   "metadata": {},
   "source": [
    "## LlamaIndex vs LangChain\n",
    "For now, both tools may look the same. One difference can easily be seen in LlamaIndex' ability to use a tree structure. The Tree Index allows you to build a hierarchical structure, where documents are segmented into chunks, and each chunk is stored as a node in a tree. This allows you to retrieve documents while keeping the context (the spot in the hierarchy where the data appeared). By contrast, LangChain operates on a flat structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b5cb841b-b984-495a-8c1e-d49d55cf47e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Root Node IDs: ['e0a8a522-1af8-467d-a3c9-a248011e14f1', '6ca5b655-2c56-4e0f-a8eb-a81835b73988']\n",
      "Node ID: e0a8a522-1af8-467d-a3c9-a248011e14f1\n",
      "Text: IEEE Std 802.11ae-2012 AMENDMENT 1: PRIORITIZATION OF MANAGEMENT FRAMES\n",
      "28 Copyright © 2012 IEEE. Al...\n",
      "Node ID: 6ca5b655-2c56-4e0f-a8eb-a81835b73988\n",
      "Text: AMENDMENT 1: PRIORITIZATION OF MANAGEMENT FRAMES IEEE Std 802.11ae-2012\n",
      "Copyright © 2012 IEEE. All r...\n"
     ]
    }
   ],
   "source": [
    "# Example tree index structure with llamaindex\n",
    "\n",
    "#!pip install llama-index-llms-ollama\n",
    "\n",
    "from llama_index.core import TreeIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Initialize the Ollama LLM with Llama3\n",
    "llama_llm = Ollama(model=\"llama3\", timeout=500)\n",
    "\n",
    "# Load a specific PDF file using SimpleDirectoryReader\n",
    "pdf_path = \"docs/802.11ae-2012_2.pdf\"\n",
    "documents = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n",
    "\n",
    "# Create a hierarchical Tree Index using Llama3 from Ollama\n",
    "tree_index = TreeIndex.from_documents(documents, llm=llama_llm)\n",
    "\n",
    "# Access the storage context from the tree index\n",
    "storage_context = tree_index.storage_context\n",
    "\n",
    "# Function to print the tree structure\n",
    "def print_tree_structure(node_id, level=0):\n",
    "    try:\n",
    "        # Retrieve the node object using the node_id from the storage context\n",
    "        node = storage_context.docstore.get_document(node_id)  # Use the exact UUID as node_id\n",
    "        \n",
    "        # Print the node name/text with indentation\n",
    "        indent = \"  \" * level  # Adjust indentation\n",
    "        print(f\"{indent}Node ID: {node_id}\")\n",
    "        print(f\"{indent}Text: {node.text[:100]}...\")  # Print the first 100 characters of the node text\n",
    "        \n",
    "        # Recursively print child nodes if they exist\n",
    "        if hasattr(node, \"child_ids\") and node.child_ids:\n",
    "            for child_id in node.child_ids:\n",
    "                print_tree_structure(child_id, level + 1)\n",
    "    except ValueError:\n",
    "        print(f\"Node ID {node_id} not found in storage context\")\n",
    "\n",
    "# Access the actual UUID root node IDs from the Tree Index\n",
    "root_node_ids = list(tree_index.index_struct.root_nodes.values())\n",
    "\n",
    "# Print the list of root node IDs to confirm\n",
    "print(\"Actual Root Node IDs:\", root_node_ids)\n",
    "\n",
    "# Start printing from each root node using the actual UUIDs\n",
    "for root_node_id in root_node_ids:\n",
    "    print_tree_structure(root_node_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d115abf-e435-461b-9a64-af2094bba577",
   "metadata": {},
   "source": [
    "Could you do the same with LangChain? No, LangChain has a flat indexing structure. You 'could' use a workaround by sotring in the metadata elements such as the page number, and search the document text for elements that could look like a hierarchical element, like numbers for chapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "41bad4f1-06ed-4d64-aef3-6b5a977ef637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 metadata: {'source': 'docs/802.11ae-2012_2.pdf', 'page': 0, 'paragraph_indices': ['.2', '.1', '.122', '.2'], 'page_number': 1}\n",
      "Document 2 metadata: {'source': 'docs/802.11ae-2012_2.pdf', 'page': 1, 'paragraph_indices': ['.2', '.3', '.4', '.7', '.3', '.3'], 'page_number': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "import re\n",
    "\n",
    "# Load the PDF\n",
    "pdf_loader = PyPDFLoader(\"docs/802.11ae-2012_2.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "# Function to extract paragraph indices from the text\n",
    "def extract_paragraph_indices(text):\n",
    "    # Regular expression pattern to match paragraph numbers like 9.4.2.1, 9.4.2.2\n",
    "    paragraph_pattern = re.findall(r'\\b\\d+(\\.\\d+)+\\b', text)\n",
    "    return paragraph_pattern if paragraph_pattern else None\n",
    "\n",
    "# Loop through documents to extract metadata\n",
    "for i, doc in enumerate(documents):\n",
    "    text = doc.page_content\n",
    "    paragraph_indices = extract_paragraph_indices(text)\n",
    "    page_number = i + 1  # assuming pages are in order in 'documents'\n",
    "\n",
    "    # Add paragraph indices and page metadata\n",
    "    if paragraph_indices:\n",
    "        doc.metadata['paragraph_indices'] = paragraph_indices\n",
    "    doc.metadata['page_number'] = page_number\n",
    "\n",
    "    # Print the metadata structure for this document\n",
    "    print(f\"Document {i + 1} metadata: {doc.metadata}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70da43-aead-4dc9-b095-da5353b40aff",
   "metadata": {},
   "source": [
    "## More on Splitters\n",
    "In the previous lesson, we focused on recursive character splitter. Regardless of the framework you use, you need to spend some time understanding, and choosing the best splitter for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2c2b528a-b584-4676-b0ab-52eb886d0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing 2 libraries \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "chunk_size =20\n",
    "chunk_overlap = 5\n",
    "\n",
    "rsplit = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "csplit = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "text1 = 'abcdefghijklmnopqrstuvwxyz1234567890'\n",
    "text2 = 'a b c d e f g h i j k l m n o p q r s t u v w x y z 1 2 3 4 5 6 7 8 9 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "581cb583-a9bb-43c2-b354-32f43ae89182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrst', 'pqrstuvwxyz123456789', '567890']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsplit.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "22deba09-45ef-415a-8f1a-9454febecf79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j',\n",
       " 'i j k l m n o p q r',\n",
       " 'q r s t u v w x y z',\n",
       " 'y z 1 2 3 4 5 6 7 8',\n",
       " '7 8 9 0']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsplit.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "408d8910-c393-4afd-9f7d-f8bab8e0cc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz1234567890']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Character splitter does not do anything, because it considers by default the end of paragraph as the separator.\n",
    "csplit.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "93f30016-2b8a-4525-8ff6-da58d7c2fdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m n o p q r s t u v w x y z 1 2 3 4 5 6 7 8 9 0']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csplit.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "77b1a98d-b416-4bbe-b2c7-d873150125d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A longer text, and a more realistic split\n",
    "chunk_size =700\n",
    "chunk_overlap = 5\n",
    "\n",
    "rsplit = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "csplit = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "77caea57-5aeb-4340-86d2-b2538e9510ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"\"\" The Martians seem to have calculated their descent with amazing subtlety--their mathematical learning is evidently far in excess of ours--and to have carried out their prepara- tions with a well-nigh perfect unanimity. Had our instru- ments \n",
    "permitted it, we might have seen the gathering trouble far back in the nineteenth century. Men like Schiaparelli watched the red planet--it is odd, by-the-bye, that for count- less centuries Mars has been the star of war--but failed to interpret \n",
    "the fluctuating appearances of the markings they mapped so well. All that time the Martians must have been getting ready.\n",
    "During the opposition of 1894 a great light was seen on the illuminated part of the disk, first at the Lick Observatory, then by Perrotin of Nice, and then by other observers. English readers heard of it first in the issue of NATURE dated August 2. I am \n",
    "inclined to think that this blaze may have been the casting of the huge gun, in the vast pit sunk into their planet, from which their shots were fired at us. Peculiar markings, as yet unexplained, were seen near the site of that outbreak during the next \n",
    "two oppositions.\n",
    "The storm burst upon us six years ago now. As Mars approached opposition, Lavelle of Java set the wires of the astronomical exchange palpitating with the amazing intelli- gence of a huge outbreak of incandescent gas upon the planet. It had occurred towards \n",
    "midnight of the twelfth; and the spectroscope, to which he had at once resorted, indicated a mass of flaming gas, chiefly hydrogen, moving with an enormous velocity towards this earth. This jet of fire had become invisible about a quarter past twelve. He \n",
    "compared it to a colossal puff of flame suddenly and violently squirted out of the planet, as flaming gases rushed out of a gun. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "42f7e284-3362-4088-a7bc-bbfc56e106d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Martians seem to have calculated their descent with amazing subtlety--their mathematical learning is evidently far in excess of ours--and to have carried out their prepara- tions with a well-nigh perfect unanimity. Had our instru- ments \\npermitted it, we might have seen the gathering trouble far back in the nineteenth century. Men like Schiaparelli watched the red planet--it is odd, by-the-bye, that for count- less centuries Mars has been the star of war--but failed to interpret \\nthe fluctuating appearances of the markings they mapped so well. All that time the Martians must have been getting ready.',\n",
       " 'During the opposition of 1894 a great light was seen on the illuminated part of the disk, first at the Lick Observatory, then by Perrotin of Nice, and then by other observers. English readers heard of it first in the issue of NATURE dated August 2. I am \\ninclined to think that this blaze may have been the casting of the huge gun, in the vast pit sunk into their planet, from which their shots were fired at us. Peculiar markings, as yet unexplained, were seen near the site of that outbreak during the next \\ntwo oppositions.',\n",
       " 'The storm burst upon us six years ago now. As Mars approached opposition, Lavelle of Java set the wires of the astronomical exchange palpitating with the amazing intelli- gence of a huge outbreak of incandescent gas upon the planet. It had occurred towards \\nmidnight of the twelfth; and the spectroscope, to which he had at once resorted, indicated a mass of flaming gas, chiefly hydrogen, moving with an enormous velocity towards this earth. This jet of fire had become invisible about a quarter past twelve. He \\ncompared it to a colossal puff of flame suddenly and violently squirted out of the planet, as flaming gases rushed out of a gun.']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsplit.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "65e6d647-d0f7-47de-b4a0-3ebbce27893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk # 0, size: 610\n",
      "chunk # 1, size: 526\n",
      "chunk # 2, size: 642\n"
     ]
    }
   ],
   "source": [
    "chunks=rsplit.split_text(text3)\n",
    "for i, _ in enumerate(chunks):\n",
    "    print(f\"chunk # {i}, size: {len(chunks[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ccb37150-94e9-4681-a342-b76fe8bf419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Martians seem to have calculated their descent with amazing subtlety--their mathematical learning is evidently far in excess of ours--and to have carried out their prepara- tions with a well-nigh perfect unanimity. Had our instru- ments \n",
      "permitted it, we might have seen the gathering trouble far back in the nineteenth century. Men like Schiaparelli watched the red planet--it is odd, by-the-bye, that for count- less centuries Mars has been the star of war--but failed to interpret \n",
      "the fluctuating appearances of the markings they mapped so well. All that time the Martians must have been getting ready.\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "295f1bc9-1d7e-4df5-a3ae-55acc1a88d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if the split is smaller:\n",
    "chunk_size =300\n",
    "chunk_overlap = 5\n",
    "rsplit = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f5236361-ea41-432b-b1ca-946ae8d243f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk # 0, size: 240\n",
      "chunk # 1, size: 245\n",
      "chunk # 2, size: 121\n",
      "chunk # 3, size: 253\n",
      "chunk # 4, size: 271\n",
      "chunk # 5, size: 256\n",
      "chunk # 6, size: 254\n",
      "chunk # 7, size: 128\n"
     ]
    }
   ],
   "source": [
    "chunks=rsplit.split_text(text3)\n",
    "for i, _ in enumerate(chunks):\n",
    "    print(f\"chunk # {i}, size: {len(chunks[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7bc54330-e491-4b02-b1e9-6432f092a740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Martians seem to have calculated their descent with amazing subtlety--their mathematical learning is evidently far in excess of ours--and to have carried out their prepara- tions with a well-nigh perfect unanimity. Had our instru- ments\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "fef61cba-dc8b-4836-b6e9-1d0ca102d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if the split is smaller:\n",
    "chunk_size =100\n",
    "chunk_overlap = 5\n",
    "rsplit = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8de75261-24a5-44d7-a81e-4bf5d8d2490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk # 0, size: 92\n",
      "chunk # 1, size: 96\n",
      "chunk # 2, size: 52\n",
      "chunk # 3, size: 99\n",
      "chunk # 4, size: 95\n",
      "chunk # 5, size: 54\n",
      "chunk # 6, size: 96\n",
      "chunk # 7, size: 29\n",
      "chunk # 8, size: 98\n",
      "chunk # 9, size: 95\n",
      "chunk # 10, size: 61\n",
      "chunk # 11, size: 97\n",
      "chunk # 12, size: 90\n",
      "chunk # 13, size: 73\n",
      "chunk # 14, size: 16\n",
      "chunk # 15, size: 97\n",
      "chunk # 16, size: 92\n",
      "chunk # 17, size: 74\n",
      "chunk # 18, size: 97\n",
      "chunk # 19, size: 96\n",
      "chunk # 20, size: 69\n",
      "chunk # 21, size: 93\n",
      "chunk # 22, size: 37\n"
     ]
    }
   ],
   "source": [
    "chunks=rsplit.split_text(text3)\n",
    "for i, _ in enumerate(chunks):\n",
    "    print(f\"chunk # {i}, size: {len(chunks[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "eae8fec5-4d9f-40fc-ae1f-fcbf712408cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Martians seem to have calculated their descent with amazing subtlety--their mathematical\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "41a05a12-598e-493e-9d68-034fca398755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "abcdefghijklmnopqrstuvwxyz1234567890\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LlamaIndex has different splitters, including sentence splitter, that makes splits based on sentences (period). \n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Initialize the SentenceSplitter with your chunk size and overlap\n",
    "ssplit = SentenceSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Apply the splitter on your text\n",
    "split_texts = ssplit.split_text(text1)\n",
    "\n",
    "# Display the split chunks\n",
    "for idx, chunk in enumerate(split_texts):\n",
    "    print(f\"Chunk {idx + 1}:\")\n",
    "    print(chunk)\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5654be-1330-41cf-add3-196a4e1ff777",
   "metadata": {},
   "source": [
    "Both frameworks have splitters with similar goals, including for LangChain (beyond CharacterSplitter and RecursiveCharacterSplitter) HTML spliiter (HTMLHeaderTextSplitter, to chop HTNL pages while following the page structure), Code splitter (RecursiveCharacterSplitter with the option e.g. language=language.PYTHON), recursive JSON splitter (RecursiveJSONSplitter), Semantic splitter (SemanticChunker, splits in sentences and uses LLM to try to find semantic structures to group chunks that form cohernet semantic ensembles) and tokens, and for LlamaIndex (beyond SentenceSplitter and TokenTExtSplitter), HTML (HTMLNodeParser), JSON (JSONNodeParser), Markdown (MarkdownNodeParser), Code (CodeSplitter with the option e.g. language=\"python\"), Hierarchical splitting (HierarchicalNodeParser, to attmept the best hierarchy of splits based on semantic meaning) and semantic splitting (SemanticSplitterNodeParser)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83986f33-3813-4882-a5a5-3165e9dd924b",
   "metadata": {},
   "source": [
    "## More on Similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7bc96-22d7-495b-8d94-cffc1020279b",
   "metadata": {},
   "source": [
    "The goal of the retrieval phase is to select the most relevant documents. But 'relevant' may mean 'repeating the same most relevant segment', which is suboptimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "82bcca4b-9db0-4aaf-b813-6e9cbeffa10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting leftovers from previous instances, as I run this codebook often\n",
    "#tempdb.delete_collection()\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# Set the environment variable to disable tokenizers parallelism and avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Let's define a directory where we'll store the database beyond this notebook execution (and let's make sure it is emtpy, as I run this notebook often :))\n",
    "persist_directory = 'docs/chroma/'\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "!rm -rf ./docs/chroma  # remove old database files if any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "79ffc73e-8604-4cf1-8d7b-a4279577cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = [\n",
    "    \"\"\"The alien spaceships looked like flying saucers.\"\"\",\n",
    "    \"\"\"The alien spaceships were round in shape.\"\"\",\n",
    "    \"\"\"The spaceships were destroying everything.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "db23e796-e41e-48ab-93f9-b7344ed07842",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdb = Chroma.from_texts(text4, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8e806ca2-6384-4a99-837c-2ab43ebbaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What can you tell me about the alien spaceships?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d5aed283-2cbf-405e-8dd7-82d682e019c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The alien spaceships were round in shape.'),\n",
       " Document(metadata={}, page_content='The alien spaceships were round in shape.')]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218bea8-2b1f-4ce9-ad96-52d413618094",
   "metadata": {},
   "source": [
    "Similarity search points to the documents that are closest semantically to the question, which may include a lot of redundant information, and miss some key points, for example that the alien spaceships were destroying everything. Max Marginal Relevance (MMR) search improves Similarity Search by picking the top k as Similarity Search does, but returning the vectors that are farthest from each other (in this top k list), so as to maximize the diversity of information returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3f5ea679-5588-4f72-aba8-01cb33e25d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The alien spaceships were round in shape.'),\n",
       " Document(metadata={}, page_content='The alien spaceships looked like flying saucers.')]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd6456-23b3-4625-9e2e-0d6548c19c5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5685dd60-035e-4070-a910-2a25acbe0b67",
   "metadata": {},
   "source": [
    "## LangChain Chains and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b0065-ed7f-4c44-bc80-d242d72af8a0",
   "metadata": {},
   "source": [
    "LangChain offers a series of tools that can be called using chains, making the process of using multiple sources for the LLM input data very flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d0a86465-7297-4786-afd8-44841c3556de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "58d7c3c3-ecdf-4daa-9bd5-b0bca33927f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "\n",
    "# Define a prompt template, used to build a prompt from various elements\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short useful fact about {topic}\"\n",
    ")\n",
    "\n",
    "# Use Ollama with Llama3 model\n",
    "model = Ollama(model=\"llama3\")\n",
    "\n",
    "# Define the output parser, which simply takes the LLM output and displays it as a string\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c8f2d6c7-57e5-4398-928d-448330086c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With LangChain you can define a chain of elements, here the prompt (output) we built is redirected into the model (as input), the model output is redirected to the parser, which uses it as input to output... a string of what the model sent \n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a8c3e012-29d3-4d66-a503-e755184ed075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour!\\n\\nHere's a short and useful fact about Paris:\\n\\nThe city of Paris has an underground tunnel system called the Catacombs de Paris, which stretches for over 150 miles (240 km) beneath the city. While not all of it is open to the public, parts of the catacombs are accessible on guided tours, offering a unique glimpse into Paris' macabre history and a chance to explore a hidden world beneath the City of Light!\""
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the chain\n",
    "chain.invoke({\"topic\": \"Paris\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "15175a00-0045-42cc-a815-e8e9bca3ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same logic can be used in RAG. Suppose that we have two chunks.\n",
    "\n",
    "#!pip install docarray\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# Initialize the Ollama embedding model\n",
    "embedding = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "# Create the vector store with two chunks of text (for simplicity. Feel free to load a full pdf or video transcript as we did above in in the previous lessons if you prefer).\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"The Martians landed in the UK\", \"The river that flows in Paris is La Seine\"],\n",
    "    embedding=embedding\n",
    ")\n",
    "\n",
    "# Use the vector store as a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b73b6924-a613-4d8e-913d-7f51f41e98f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The river that flows in Paris is La Seine'),\n",
       " Document(metadata={}, page_content='The Martians landed in the UK')]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the retriever does when we ask a simple question - it should retrieve the available chunks, here both chunks irrespective of the question, as we do not run similarity search, just memory search (what is the the memory)\n",
    "retriever.get_relevant_documents(\"where did the Martians land?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a0e49373-a76d-4e5e-ba26-fc450ffd3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a prompt template as above, that tells the LLM to use the context (i.e. the best chunk) to answer the question\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4e8419e6-34f9-4d64-8641-5e5dc49be5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to pass the context (the 2 sentences retrieved from the vector store) and the question to the LLM. We use RunnableMap to create a dictionary with 2 elements, the context and the question\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]), # the context is retrieved by sending the question to the retriever, as we did manually 2 blocks above; the context is the first part of the dictionary\n",
    "    \"question\": lambda x: x[\"question\"] # the second element of the dictionary is the question itself \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "90b04897-2f4e-4ac5-821f-1d650e4d9d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='The river that flows in Paris is La Seine'),\n",
       "  Document(metadata={}, page_content='The Martians landed in the UK')],\n",
       " 'question': 'where did the Martians land?'}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the dictionary looks like\n",
    "inputs.invoke({\"question\": \"where did the Martians land?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b5b28f1f-9b8c-4fc9-ba3e-34baa3598452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then want to pass the content of the dictionary to the prompt template (making a prompt of it), that is sent to the model, which output is sent to the parser, that makes a string from the answer. \n",
    "# Let's define the dictionary again, this time piping it into the prompt, then to the model, then to the parser, and we call the whole chain 'chain'\n",
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]), \n",
    "    \"question\": lambda x: x[\"question\"]  \n",
    "}) | prompt | model | output_parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7f6c1ae4-3aa7-4f9f-b684-930735c87a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided documents, the Martians landed in the UK.'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens when we call the chain?\n",
    "chain.invoke({\"question\": \"where did the Martians land?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019cc068-6a5b-46a2-9308-253bc23fba03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87d0ae-927c-4ae1-92c6-dd062e56336d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494748c-4c9b-4ee2-a6bc-2913b1c88fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d6618-d5be-4d19-8fcd-85832365523a",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
